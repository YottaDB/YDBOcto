#################################################################
#								#
# Copyright (c) 2019-2020 YottaDB LLC and/or its subsidiaries.	#
# All rights reserved.						#
#								#
#	This source code contains the intellectual property	#
#	of its copyright holder(s), and is made available	#
#	under a license.  If you do not know the terms of	#
#	the license, please stop and do not read further.	#
#								#
#################################################################

init_test() {
	export test_temp=$(mktemp -d @TEST_OUTPUT_DIR@/bats-test.XXXXXX)
	echo "Temporary files in: $test_temp"
	exec >  >(tee -ia $test_temp/stdout.txt)
	exec 2> >(tee -ia $test_temp/stderr.txt >&2)
	cd $test_temp
	# Disable any user-level (~/.inputrc) customizations that can cause test failures (e.g. TERR008, TSC18 subtests)
	# For example "set editing-mode vi" in ~/.inputrc somehow re-enables tab-completion even though Octo disables it
	#	by calling `rl_bind_key('\t', rl_insert);`
	export INPUTRC=/etc/inputrc
	# Below has been seen to avoid Ctrl-M ('^M') characters in long query strings
	export COLUMNS=4096
	# Set YDB related env vars
	unset ydb_gbldir gtmgbldir	# needed or else ydb_env_set can issue ZGBLDIRACC error (due to it calling MUPIP DUMPFHEAD)
					# if ydb_gbldir is defined and points to a non-existent gld file.
	# Randomly choose whether to use UTF-8 or M mode
	source @YOTTADB_INCLUDE_DIRS@/ydb_env_unset
	if [[ $(( $RANDOM % 2)) -eq 0 ]]; then
		export ydb_chset=UTF-8
		utf8_path="/utf8"
	else
		unset ydb_chset
		utf8_path=""
		# Set ydb_icu_version to the right value even if we chose M mode. This way, wherever we set ydb_chset to "UTF-8"
		# (e.g. in "load_fixture()" function while loading the northwind.zwr fixture) things will work fine.
		# Not setting this can cause ICUSYMNOTFOUND errors. No need to do this for the case when "ydb_chset" is chosen
		# to be "/utf8" as the "ydb_env_set" call done after this if/else block will set that env var.
		export ydb_icu_version=`pkg-config --modversion icu-io`
	fi
	source @YOTTADB_INCLUDE_DIRS@/ydb_env_set
	# Run tests locally without installing Octo/Rocto to $ydb_dist
	if [[ @DISABLE_INSTALL@ == "ON" ]]; then
		export PATH="@PROJECT_BINARY_DIR@/src:$PATH"
		# tests/fixtures has lots of M programs. Make sure .o files for those get created in current test output directory
		# (and not in tests/fixtures) as it can otherwise cause .o file format issues (e.g. if YDB changes the M .o file format)
		# since tests/fixtures persists a lot longer than the test output directory. We had previously seen COLLATIONUNDEF errors
		# while trying to use the master branch of YDB repo when .o files created by r1.29 in Nov 2019 were tried to be used
		# by r1.29 in Dec 2019 (after the .o file format was changed in between). Having the .o files in the test output directory
		# avoids such issues. This comment also applies to the similar line in the below `else` block.
		ydbroutines=".(. @PROJECT_SOURCE_DIR@/tests/fixtures) @PROJECT_BINARY_DIR@/src$utf8_path/_ydbocto.so @PROJECT_BINARY_DIR@"
	else
		# Add the UTF-8 path, if set, as this is not done by build.sh in the pipeline
		ydbroutines=".(. @PROJECT_SOURCE_DIR@/tests/fixtures)"
		ydbroutines="$ydbroutines .(. @YOTTADB_INCLUDE_DIRS@/plugin/r/) @PROJECT_BINARY_DIR@"
	fi
	export ydb_routines="$ydbroutines $ydb_routines"
	# Log env vars, shell vars, locale info in files for later analysis (if needed). Mask any sensitive env vars out.
	echo " --> Running test [$BATS_TEST_FILENAME] : subtest [$BATS_TEST_DESCRIPTION] : in $PWD : build type @CMAKE_BUILD_TYPE@ " > bats_test.out
	env | grep -vE "HUB_USERNAME|HUB_PASSWORD|CI_JOB_TOKEN|CI_REGISTRY_PASSWORD|CI_BUILD_TOKEN|CI_REPOSITORY_URL" > env.out
	set | grep -vE "HUB_USERNAME|HUB_PASSWORD|CI_JOB_TOKEN|CI_REGISTRY_PASSWORD|CI_BUILD_TOKEN|CI_REPOSITORY_URL" > set.out
	locale > locale.out
	locale -a > localeall.out
	# Set env var to allow .m and .o file timestamps to be identical. This is expected to be particularly useful
	# in the pipeline test runs where we have seen the _ydboctoP*.o file almost always created with a timestamp
	# that is 1 second later than the corresponding _ydboctoP*.m file. We suspect this is due to the underlying
	# file system granularity being at 1-second (instead of 1-milli/micro/nano second). Once this env var is
	# set, YDB will not wait for the .o file time stamp to be different than the .m file. Since there are almost
	# thousands of queries that run in the pipeline tests lots of these 1-second delays adds up to minutes of
	# slowdown which should all hopefully go away with this env var set.
	export ydb_recompile_newer_src=TRUE
	# Below is needed to avoid wildcard expansion order (e.g. cat *.m used in various tests) changing based on LC_TYPE
	# It is also needed to avoid sort order changing (and reference file issues) based on LC_CTYPE being "C" or "en_US.UTF8".
	# Fixing LC_ALL to a particular value ensures caller environment does not affect test results.
	export LC_ALL=en_US.UTF8
	# Avoid generating .pyc files during tests, which clutter up the source directory
	# All the python scripts take a very small fraction of the test time,
	# so this does not impact pipeline times in any significant way.
	export PYTHONDONTWRITEBYTECODE=1
}

init_tls() {
	# Generate CA key and certificate
	openssl genpkey -algorithm RSA -pkeyopt rsa_keygen_bits:2048 -pass pass:tester -out $test_temp/CA.key
	openssl req -new -nodes -key $test_temp/CA.key -passin pass:tester -days 365 -x509 \
			-subj "/C=US/ST=PA/L=Malvern/O=Octo/CN=www.yottadb.com" -out $test_temp/CA.crt
	# Create server key and certificate request
	openssl genpkey -algorithm RSA -pkeyopt rsa_keygen_bits:2048 -pass pass:tester -out $test_temp/server.key
	openssl req -new -key $test_temp/server.key -passin pass:tester \
		-subj "/C=US/ST=PA/L=Malvern/O=Octo/CN=www.yottadb.com" -out $test_temp/server.csr
	# Sign certificate based on request and local CA
	openssl x509 -req -in $test_temp/server.csr -CA $test_temp/CA.crt -CAkey $test_temp/CA.key -CAcreateserial \
		-out server.crt -days 365
	# Pass private key password to environment variable
	echo tester | $ydb_dist/plugin/gtmcrypt/maskpass | cut -f 3 -d " " >> env.log
	export ydb_tls_passwd_OCTOSERVER=$(echo tester | $ydb_dist/plugin/gtmcrypt/maskpass | cut -f 3 -d " ")
	export ydb_crypt_config=$test_temp/octo.conf
	cat <<OCTO &>> $test_temp/octo.conf
rocto: {
	ssl_on: true;
}

tls: {
	CAfile: "$test_temp/CA.crt";
	CApath: "$test_temp/";
	OCTOSERVER: {
		format: "PEM";
		cert: "$test_temp/server.crt";
		key: "$test_temp/server.key";
	}
}
OCTO
}

copy_test_files() {
	for f in $@; do
		mkdir -p $test_temp/$(dirname $f)
		cp @PROJECT_SOURCE_DIR@/tests/$f $test_temp/$f
	done
}

# load_fixture <fixture name, relative to tests/fixtures
load_fixture() {
	fixture_name=$1
	echo "Loading fixture $fixture_name"
	if [[ $fixture_name == *.zwr ]]; then
		# Determine the chset (M or UTF-8) to use from the extract file. It will be either "UTF-8" or "".
		chset=`grep "MUPIP EXTRACT" @PROJECT_SOURCE_DIR@/tests/fixtures/$fixture_name | awk '{print $4}'`
		# Keep LC_ALL as en_US.UTF8 (set in "init_test()" function) for both M and UTF-8 mode. That should work just fine.
		ydb_chset=$chset $ydb_dist/mupip load @PROJECT_SOURCE_DIR@/tests/fixtures/$fixture_name
	elif [[ $fixture_name == *.sql ]]; then
		if [[ $2 == "subtest" ]]; then
			cp @PROJECT_SOURCE_DIR@/tests/fixtures/$fixture_name $fixture_name
			grep -v '^#' $fixture_name 2>&1 | tee -a output.txt	   # Filter out copyright from output
			grep -vE '^-- |^#' $fixture_name > input.sql || [ $? = 1 ] # Filter out comment lines as they otherwise clutter the parse error output (if any)
			if [[ $3 == "novv" ]]; then
				octoflags=""
			elif [[ $3 =~ "v" ]]; then	# Allow verbosity level to be passed directly
				octoflags="-$3"
			else
				octoflags="-vv"
			fi
			octo $octoflags -f input.sql 2>&1 | tee -a output.txt
		else
			octo -f @PROJECT_SOURCE_DIR@/tests/fixtures/$fixture_name
		fi
	else
		echo "Unrecognized file extension: $fixture_name"
		exit 1
	fi
}

create_postgres_database() {
	psql postgres <<PSQL &> $1.setup.out
	-- Need to use LC_COLLATE='C' to ensure string comparison of 'Z' < 'a' returns TRUE (default is en_US.UTF8)
	-- Need template=template0 to avoid the following error
	-- "new collation (C) is incompatible with the collation of the template database (en_US.UTF-8)"
	create database $1 LC_COLLATE='C' template=template0;
PSQL
	# Create a PostgreSQL role for the user running the tests and grant that user sufficient PostgreSQL permissions to
	# log in to the specified database and access all the tables there. Note that ALTER is used for the case when the user
	# already exists in PostgreSQL, but doesn't have the required settings.
	psql $1 <<PSQL 2>&1 | tee -a $1.setup.out
	CREATE ROLE $USER;
	ALTER ROLE $USER LOGIN;
	ALTER ROLE $USER PASSWORD 'ydbrocks';
	GRANT CONNECT ON DATABASE $1 TO $USER;
	GRANT ALL PRIVILEGES ON ALL TABLES IN SCHEMA public TO $USER;
PSQL
}

load_postgres_fixture() {
	# If the table already exists, the psql command will return with a non-zero exit status due to the
	# `\set ON_ERROR_STOP on` usage in the .sql script. But that is not a real error and so we use the `run` command
	# below to avoid bats from treating this as an error.
	psql $1 -f @PROJECT_SOURCE_DIR@/tests/fixtures/$2 &> $1.psql.load || true
}

# Load a large amount of dummy data for tests requiring slow queries
load_big_data() {
	$ydb_dist/mumps -run ^%XCMD 'for i=1:4:1000000 s ^names(i+5)=(i+5)_"|A|B"'
	$ydb_dist/mumps -run ^%XCMD 'for i=2:4:1000000 s ^names(i+5)=(i+5)_"|C|B"'
	$ydb_dist/mumps -run ^%XCMD 'for i=3:4:1000000 s ^names(i+5)=(i+5)_"|A|C"'
	$ydb_dist/mumps -run ^%XCMD 'for i=4:4:1000000 s ^names(i+5)=(i+5)_"|B|A"'
}

createdb() {
	export ydb_gbldir="mumps.gld"
	echo "ydb_gbldir: $ydb_gbldir"
	# Create two regions by default. One to hold application/user data. One to hold Octo internal data (%ydbocto* namespace)
	# But if user specified number of regions explicitly (only allowed value currently is 1) then create 1 region only.
	if [[ $1 == "1" ]]; then
		$ydb_dist/mumps -r ^GDE <<FILE
		change -r DEFAULT -key_size=1019 -record_size=1048576
		change -segment DEFAULT -file_name=$test_temp/mumps.dat
		change -r DEFAULT -NULL_SUBSCRIPTS=true
		exit
FILE
	else
		$ydb_dist/mumps -r ^GDE <<FILE
change -region DEFAULT -null_subscripts=true -record_size=1048576
change -segment DEFAULT -file_name=$test_temp/mumps.dat
add -name %ydbocto* -region=OCTOREG
add -region OCTOREG -dyn=OCTOSEG
add -segment OCTOSEG -file=$test_temp/octo.dat
change -region OCTOREG -null_subscripts=true -key_size=1019 -record_size=1048576
exit
FILE
	fi
	rm *.dat || true
	$ydb_dist/mupip create
	echo "Populating seed data"
	octo -f @PROJECT_BINARY_DIR@/octo-seed.sql
	ydb_chset="" $ydb_dist/mupip load  @PROJECT_BINARY_DIR@/octo-seed.zwr
	# Set the below env var to ensure no DBFILEXT messages show up in syslog and in turn in individual test output files
	# e.g. if a test redirects stderr to a file output.txt, then syslog messages would show up in that file if run through
	# the pipeline causing a test failure if that is compared against a reference file (e.g. TC001 in test_create_table.bats.in)
	export ydb_dbfilext_syslog_disable=1
}

gde_add_region() {
	$ydb_dist/mumps -r ^GDE <<FILE
add -name $2 -region=$1
add -region $1 -dynamic=$1 -key_size=1019 -record_size=1048576
add -segment $1 -file_name=$test_temp/$1.dat
exit
FILE
	$ydb_dist/mupip create -region=$1
}

gde_add_name() {
	echo "NAME: $1"
	$ydb_dist/mumps -r ^GDE <<FILE
add -name $1 -region=$2
exit
FILE
}

set_null_subs() {
	$ydb_dist/mupip set -region $1 -NULL_SUBSCRIPTS=$2
}

find_open_port() {
	if [[ $1 =~ ^[-+]?[0-9]+$ ]]; then
		open_port=$1
	else
		open_port=1337
	fi
	netstat -tulpn | grep ":$open_port" &> /dev/null
	result=$?
	while [[ $result -eq 0 ]]; do
		open_port=$(($open_port+1))
		netstat -tulpn | grep ":$open_port" &> /dev/null
		result=$?
	done
	echo $open_port
}

start_rocto() {
	create_user $USER "ydbrocks" &> /dev/null
	echo "ROCTO START" >> env.log
	env >> env.log
	if [[ $2 == "quiet" ]]; then
		verbosity="-v"
		opt=$3
	elif [[ $2 == "verbose" ]]; then
		verbosity="-vvv"
		opt=$3
	else
		verbosity="-vv"
		# Allow optional second argument
		opt=$2
	fi
	rocto_port=$(find_open_port $1)
	rocto $verbosity -p $rocto_port $opt &> rocto.log &
	echo $! > rocto.pid
	while [[ ! -e rocto.log || "$(grep -c "rocto started" rocto.log)" == "0" ]]; do
		sleep .1s
	done
	echo $rocto_port
}

stop_rocto() {
	# Wait for rocto listener to die
	if [[ -e rocto.pid ]]; then
		listener_pid=$(cat rocto.pid)
		# Wait for rocto servers (i.e. children of the listener) to die
		ps --no-headers --ppid $listener_pid | awk '{print $1}' | while read server_pid; do
			if [[ -d /proc/$server_pid ]]; then
				# Wait for rocto process to actually die
				while [[ -d /proc/$server_pid ]]; do
					sleep .1s
				done
			fi
		done
		if [[ -d /proc/$listener_pid ]]; then
			$ydb_dist/mupip stop $listener_pid
			# Wait for rocto process to actually die
			while [[ -d /proc/$listener_pid ]]; do
				sleep .1s
			done
		fi
	fi
}

run_psql() {
	if [[ $2 =~ ".sql" ]]; then
		# Load a fixture
		strip_sql_comments $2
		PGPASSWORD=ydbrocks psql $3 -U ydb "host=localhost port=$1" < $2
	else
		PGPASSWORD=ydbrocks psql $3 -U ydb "host=localhost port=$1"
	fi
}

run_psql_auth() {
	if [[ @YDB_TLS_AVAILABLE@ -eq 1 ]]; then
		PGPASSWORD=$2 psql --no-align -U $1 "host=localhost port=$3"
	else
		PGPASSWORD=$2 psql --no-align -U $1 "sslmode=disable host=localhost port=$3"
	fi
}

run_psql_expect() {
	unset PGPASSWORD
	(expect -d -f @PROJECT_SOURCE_DIR@/tests/fixtures/$1.exp $2 > expect.out) &> expect.dbg
}

create_user() {
	echo -en "$2\n$2" | yottadb -r %ydboctoAdmin add user $1
}

delete_users() {
	for user in $@; do
		yottadb -r %ydboctoAdmin delete user $user
	done
}

setup_go() {
	mkdir -p "$test_temp/go/src/"
	export GOPATH="$test_temp/go"
}

run_go() {
	cp -r @PROJECT_SOURCE_DIR@/tests/go/src/$1 $GOPATH/src/$1
	go get $1
	go build $1
	$GOPATH/bin/$1 $2
	# Executables created by go occupy a noticeable part of the artifacts.zip file created in the pipeline.
	# They are not considered necessary at this point for test failure analysis so remove them.
	rm -f $1
}

run_java() {
	# Check if .java file has already been compiled to a .class file. If so skip that step.
	if [[ ! -e @PROJECT_BINARY_DIR@/$1.class ||
		$(stat -c '%Y' @PROJECT_BINARY_DIR@/$1.class) -le $(stat -c '%Y' @PROJECT_SOURCE_DIR@/tests/fixtures/$1.java) ]]; then
		# Copy .java file from fixtures directory to current directory to create .class file
		cp @PROJECT_SOURCE_DIR@/tests/fixtures/$1.java @PROJECT_BINARY_DIR@
		# Compile the .java file to a .class file
		javac @PROJECT_BINARY_DIR@/$1.java	# will create the file $1.class

	fi
	strip_sql_comments $3
	if [[ $1 == "run_query" ]]; then
		# Only append ".sql" for queries derived from fixtures, not for generated queries
		if [[ -f $3 ]]; then
			query=$(cat $3)
		else
			query=$(cat $3.sql)
		fi
	elif [[ $1 == "run_multi_query" ]]; then
		query=$3.sql
	elif [[ $1 == "run_multiple_query_files" ]]; then
		query=$3
	else
		query=""
	fi
	psql_jar="@PROJECT_BINARY_DIR@/postgresql-$JDBC_VERSION.jar"
	if ! [[ -v JDBC_VERSION ]]; then
		echo "error: \$JDBC_VERSION was required but not set. Consult the Octo README for more information."
	elif ! [[ -e "$psql_jar" ]]; then
		echo "error: $psql_jar does not exist. Cannot continue."
	fi
	# Run the $1.java file now that the $1.class file has been compiled/created.
	# Ensure both the postgresql-$JDBC_VERSION.jar file and the $1.class file will be found in the -classpath specification below
	java -classpath @PROJECT_BINARY_DIR@/postgresql-$JDBC_VERSION.jar:@PROJECT_BINARY_DIR@ $1 $2 "$query" $4 $5 $6
}

strip_psql_header() {
	sed -i '/Password for user /,/^[[:space:]]*$/d' $1
}

strip_expect_artifacts() {
	sed -i '/^.*spawn \/bin\/bash.*$/d' $1
	sed -i '/^.*stty cols 4096.*$/d' $1
	sed -i '/^.*psql -U ydb -h localhost -p .*$/d' $1
	# Shell prompt would have the bats-test.* directory name in some platforms (e.g. CentOS).
	# Shell prompt would have the directory name as $PWD in some platforms (e.g. Ubuntu)
	# Account for that in the 2 lines below. The [#$] is to account for either a # or a $ as the shell prompt
	sed -i '/^.*bats-test\..*[#$].*$/d' $1
	sed -i '/^.*\$PWD[#$]*.*$/d' $1
}

# Filter out '#' comment lines and blank lines, if any
strip_sql_comments() {
	# Remove .sql extension (occurs when root caller is run_query_in_octo_and_postgres_and_crosscheck)
	filename=$(echo $1 | cut -d'.' -f 1)
	# Comments are only present for fixtures
	if [[ -f @PROJECT_SOURCE_DIR@/tests/fixtures/$filename.sql ]]; then
		cp @PROJECT_SOURCE_DIR@/tests/fixtures/$filename.sql $filename.in
		grep -v '^#' $filename.in | grep -v '^[[:space:]]*$' > $filename.sql
	fi
}

verify_output() {
	echo "Comparing outref/$1 $2"
	copy_test_files outref/$1.ref
	cp $2 clean_output.txt

	# Universal filters
	# Filter time and dates
	sed -i 's/[0-9]\{4\}-[0-9]\{2\}-[0-9]\{2\}/DATE/g' clean_output.txt
	sed -i 's/[0-9]\{2\}:[0-9]\{2\}:[0-9]\{2\}/TIME/g' clean_output.txt
	# Filter file path
	sed -i 's/\/.*\/.*\.[yc]:[0-9]*/PATH:LINENUM/' clean_output.txt
	# Filter rule #s and line #s printed by flex in TRACE verbosity level output
	sed -i 's/Entering state [0-9]*/Entering state sss/' clean_output.txt
	sed -i 's/Stack now [0-9 ]*/Stack now sss .../' clean_output.txt
	sed -i 's/\(Reducing stack by rule\) \([0-9]*\) \((line\) [0-9]*)/\1 rrr \3 lll)/' clean_output.txt
	# Filter octo.conf from $ydb_dist and $HOME, i.e. not in the local directory
	# Cannot specify $ydb_dist directly as it varies depending on UTF-8 mode selection,
	# so just use "plugin/octo" as that is guaranteed to be in the $ydb_dist path.
	# Note also that the $ydb_dist case is unconditionally removed - otherwise this
	# message will cause outref discrepancies for the two DISABLE_INSTALL cases ("ON" and "OFF").
	sed -i "/\/.*\/plugin\/octo\/octo.conf/d" clean_output.txt
	sed -i "s,$HOME\/octo.conf,\$HOME\/octo.conf," clean_output.txt
	# Filter M routine name
	sed -i 's/_ydboctoP.*\.m/_ydboctoP*.m/' clean_output.txt
	sed -i 's/_ydboctoX.*\.m/_ydboctoX*.m/' clean_output.txt
	# Filter M routine name with %
	sed -i 's/\%ydboctoP[a-zA-Z0-9]*,/%ydboctoP*,/' clean_output.txt
	# Filter OBJ file name
	sed -i 's/_ydboctoP.*\.o/_ydboctoP*.o/' clean_output.txt
	# Filter cursor number
	sed -i 's/cursor [0-9]*/CURSOR_NUM/' clean_output.txt
	# Filter ydb_* environment variables
	sed -i '/^.*\[ INFO\] PATH:LINENUM DATE TIME : # .*$/d' clean_output.txt
	# Convert non-deterministic $PWD into a deterministic one
	# Convert symbolic links into absolute paths
	cd -P $PWD
	sed -i 's,'$PWD',$PWD,g' clean_output.txt
	# Convert PROJECT_SOURCE_DIR references to generic one
	sed -i 's,'@PROJECT_SOURCE_DIR@',SRCDIR,g' clean_output.txt
	# Convert YDB C source file/line references to generic one
	sed -i 's,\(called from module\) .*/\(sr_.*\) at line [0-9]*,\1 \2 at line LINE,' clean_output.txt
	# Filter version number
	sed -i 's/Octo version [0-9]\.[0-9]\.[0-9]/Octo version x\.x\.x/' clean_output.txt
	sed -i 's/Rocto version [0-9]\.[0-9]\.[0-9]/Rocto version x\.x\.x/' clean_output.txt
	# Filter git commit hash
	sed -i 's/Git commit: [0-9,a-f]*/Git commit: xxxx/' clean_output.txt
	# Filter git uncommitted changes
	sed -i 's/Uncommitted changes: .*/Uncommitted changes: false/' clean_output.txt
	# Filter forked process pid
	sed -i 's/rocto server process forked with pid [0-9]*/rocto server process forked with pid PID/' clean_output.txt
	# Convert psql prompt to root for compatibility when running tests on systems with different PostgreSQL user permissions
	# Specifically, some test machines do not run tests with full PostgreSQL user permissions and so receive a prompt with '>' instead of '#'
	sed -i 's/=>/=#/' clean_output.txt

	# Selectively process output
	for i in "$@"; do
		# Filter verbosity statements from octo and psql
		# a default octo install will have WARNING verbosity so INFO and DEBUG will need to be filtered for most tests
		if [[ $i == "noinfo" ]]; then
			sed -i '/\[ INFO\]\|^INFO:/d' clean_output.txt
		fi
		if [[ $i == "nodebug" ]]; then
			sed -i '/\[DEBUG\]\|^DEBUG:/,/^[[:space:]]*$/d' clean_output.txt
		fi
		if [[ $i == "nosenderror" ]]; then
			sed -i '/failed to send message/d' clean_output.txt
		fi
		if [[ $i == "noconnclose" ]]; then
			# Remove "connection closed cleanly" messages
			sed -i '/connection closed cleanly/d' clean_output.txt
		fi
		if [[ $i == "noprompt" ]]; then
			# Remove octo prompt, but keep remainder of line
			sed -i 's/^.*OCTO>//' clean_output.txt
		fi
		if [[ $i == "nopromptline" ]]; then
			# Remove octo prompt lines
			sed -i '/OCTO>/,$!d' clean_output.txt
		fi
		if [[ $i == "psql" ]]; then
			# filter socket info
			sed -i 's/\[[0-9]*\.[0-9]*\.[0-9]*\.[0-9]*:[0-9]*\]/\[SOCKET\]/' clean_output.txt
		fi
		if [[ $i == "noport" ]]; then
			# filter port from INFO messages in rocto
			sed -i 's/rocto started on port [0-9]*/\[PORT\]/' clean_output.txt
		fi
		if [[ $i == "noexpect" ]]; then
			# Remove expect script artifacts
			strip_expect_artifacts clean_output.txt
			strip_psql_header clean_output.txt
		fi
		if [[ $i == "stripreturns" ]]; then
			sed -i 's/\r//g' clean_output.txt
		fi
		if [[ $i == "striphex" ]]; then
			# Remove any hex values consisting of 2 to 8 hex characters (16-64 bits)
			sed -i 's/0x[0-9a-fA-F]\{2,16\}/HEX/' clean_output.txt
		fi
		if [[ $i == "noforcedhalt" ]]; then
			sed -i '/%YDB-F-FORCEDHALT/d' clean_output.txt
		fi
		if [[ $i == "noconfig" ]]; then
			# Remove Octo's "Loading config from ..." message while leaving other INFO messages intact
			sed -i '/Loading config from/d' clean_output.txt
		fi
		# The sort argument should be passed last to allow sorting of final output file
		if [[ $i == "sort" ]]; then
			cat clean_output.txt | sort -o clean_output.txt
		fi
	done
	# test_where_in.bats TWI12 when executed with JDBC driver fails without the following code.
	# This is because the semicolon at the end of a query is left out of the physical plan in this case.
	# To avoid such failure we remove all occurrences of semicolons before the comparison.
	local isjdbcclient=$(grep "randclient:JDBC" env.out)
	if [[ ! -z "$isjdbcclient" ]]; then
		sed -i 's/;//g' outref/$1.ref
		sed -i 's/;//g' clean_output.txt
	fi
	diff outref/$1.ref clean_output.txt
	return
}

count_for_loops() {
	numloops=$(grep -c "FOR " $1) || true	# grep -c can return non-zero status in case of no match hence the need for "|| true"
	if [[ $numloops -ne $2 ]]; then
		echo "Expected $2 FOR loops but found $numloops loops instead in $1"
		return -1
	fi
	return
}

count_num_occurrences() {
	numoccurrences=$(grep -c "$1" $2) || true # grep -c can return non-zero status in case of no match hence the need for "|| true"
	if [[ $numoccurrences -ne $3 ]]; then
		echo "Expected $3 occurrences of [$1] but found $numoccurrences occurrences instead in $2"
		return -1
	fi
	return
}

run_query_in_octo_and_postgres_and_crosscheck_multiple_queries() {
	# Query file has multiple queries.
	# Split it into multiple query files each containing one query and invoke "run_query_in_octo_and_postgres_and_crosscheck()".
	# Spliting queries are needed as otherwise Postgres and Octo output cannot be easily compared.

	# Init
	database="$1"
	queryfile="$2"
	jdbcprotocol=usesimple
	psql_port="5432"
	local num_clients
	fname=`echo $queryfile | sed 's/\..*//g'`
	# psql errors will never have the same output as octo errors, so they cannot be used in crosscheck.
	# Therefore any error should fail the test.
	# However, on CentOS 7 there are many spurious failures due to postgres not being able to infer the type of NULL.
	# Ignore these failures because they do not affect the correctness of Octo.
	# Postgres versions under 10 throw "failed to find conversion function from unknown to text" error
	# when queries such as ".. WHERE a.firstName = (SELECT NULL);" access postgres through JDBC driver.
	# To avoid such failures the JDBC client is excluded in such a case.
	postgres_version="$(psql --command='show server_version;' --no-align --tuples-only postgres)"
	local psql_cmd
	if [ "$(echo "$postgres_version" | cut -d . -f 1)" -lt 10 ]; then
		psql_cmd=psql
		num_clients=2
	else
		psql_cmd="psql -v ON_ERROR_STOP=1"
		num_clients=3
	fi

	# Process parameters
	for i in "$@"; do
		if [[ $i == "trim_trailing_zeroes" ]]; then
			trimzeroes="trim_trailing_zeroes"
		fi
		if [[ $i == "usejdbc" ]]; then
			usejdbc="usejdbc"
		fi
		if [[ $i == "useextended" ]]; then
			jdbcprotocol="useextended"
		fi
		if [[ $i == "useocto" ]]; then
			randclientstr="OCTO"
		fi
	done
	if [[ $randclientstr != "OCTO" && $usejdbc != "usejdbc" ]]; then
		# Randomly select client
		randclient=$(( $RANDOM % $num_clients))
		# Setup client parameters based on the selection
		if [[ $randclient == 2 ]]; then
			randclientstr="JDBC"
			jdbcprotocol=$(( $RANDOM % 2))
			if [[ $jdbcprotocol == 0 ]]; then
				jdbcprotocol=usesimple
			else
				jdbcprotocol=useextended
			fi
			if [[ -z "$test_port" ]]; then
				# test_port not set in subtest, set isnotexternal to
				# inform further script to stop_rocto on test completion or failure
				isnotexternal=1
				test_port=$(start_rocto 1344)
			fi
		elif [[ $randclient == 1 ]]; then
			randclientstr="PSQL"
			load_fixture default_user.zwr
			if [[ -z "$test_port" ]]; then
				# test_port not set in subtest, set isnotexternal to
				# inform further script to stop_rocto on test completion or failure
				isnotexternal=1
				test_port=$(start_rocto 1339)
			fi
		else
			randclientstr="OCTO"
		fi
	fi
	echo "Cross check parameters:" >> env.out
	echo "postgres_version:$postgres_version randclient:$randclientstr usejdbc:$usejdbc jdbcprotocol:$jdbcprotocol" >> env.out

	# Prepare query file/files for execution
	if [[ ! -f $queryfile ]]; then
		cp @PROJECT_SOURCE_DIR@/tests/fixtures/$queryfile .
	fi
	fname=`echo $queryfile | sed 's/\..*//g'`

	if [[ $randclientstr == "JDBC" || $usejdbc == "usejdbc" ]]; then
		# Remove commented instructions like -- sort-needed-check or -- rowcount-only-check
		# and commented queries as they are not handled well by JDBC client
		sed 's/\s*--.*//' $queryfile &> "$fname"_nocomment.sql
		mv $queryfile "$fname"_comment.sql
		# Maintaining two copies of the query file with one having comments is required to facilitate
		# result comparison in run_query_verify.
		@PROJECT_SOURCE_DIR@/tests/fixtures/sqllogic/split_queries.py "$fname"_comment.sql
		@PROJECT_SOURCE_DIR@/tests/fixtures/sqllogic/split_queries.py "$fname"_nocomment.sql
		# Only $fname is passed as it acts as a search pattern for run_multiple_query_files
		# to find all query files.
		run_query_in_octo_and_postgres_and_crosscheck "$fname"
	else
		@PROJECT_SOURCE_DIR@/tests/fixtures/sqllogic/split_queries.py $queryfile
		for splitfile in $fname-*.sql
		do
			run_query_in_octo_and_postgres_and_crosscheck $splitfile
		done

	fi

	# Stop Rocto when started by the test framework in parameter processing phase
	run_query_stop_rocto
	return
}

run_query_in_octo_and_postgres_and_crosscheck() {
	if [[ $randclientstr == "JDBC" || $usejdbc == "usejdbc" ]]; then
		local fname="$1"
	else
		query_file="$1"
		local fname=`echo $query_file | sed 's/\..*//g'`
	fi
	if [[ $usejdbc == "usejdbc" || $randclientstr == "JDBC" ]]; then
		run_java run_multiple_query_files $psql_port "$fname"_nocomment- "$jdbcprotocol" ".psql.out" $database
		run_java run_multiple_query_files $test_port "$fname"_nocomment- "$jdbcprotocol" ".octo.out"
		local query_file
		for nocomment_query_file in "$fname"_nocomment-*.sql
		do
			# Postgres displays boolean values as t and f whereas Octo displays them as 1 and 0 so fix that before the diff.
			cat $nocomment_query_file.psql.out | sed 's/\<t\>/1/g;s/\<f\>/0/g;' >& $nocomment_query_file.ref
			query_num=`echo $nocomment_query_file | sed 's/.*_nocomment-//g'`
			query_file="$fname"_comment-"$query_num"
			# Distinction between commented and non-commented query file is required
			# because output processing utilizes commented instructions for Octo Postgres comparison
			run_query_verify $query_file $nocomment_query_file
		done
	else
		$psql_cmd --no-align $database -f $query_file >& $fname.psql.out
		# Postgres displays boolean values as t and f whereas Octo displays them as 1 and 0 so fix that before the diff.
		tail -n +2 $fname.psql.out | head -n -1 | sed 's/\<t\>/1/g;s/\<f\>/0/g;' >& $fname.ref
		if [[ $randclientstr == "PSQL" ]]; then
			run_psql $test_port $query_file --no-align > $fname.octo.out.tmp 2> $fname.octo.out.err
			# trim header and footer information from psql output for effective comparison
			tail -n +2 $fname.octo.out.tmp | head -n -1 | sed 's/\<t\>/1/g;s/\<f\>/0/g;' &> $fname.octo.out
		else
			# OCTO
			octo -f $query_file >& $fname.octo.out
		fi
		run_query_verify $query_file $fname
	fi
	return
}

run_query_verify() {
	local query_file="$1"
	local fname="$2"
	sed -i '/^.*\[ INFO\].* \/.*\/octo.conf/d' $fname.octo.out
	cat $fname.octo.out >& $fname.log
	if [[ $trimzeroes == "trim_trailing_zeroes" ]]; then
		# Remove floating point numbers with extraneous 0s at the end
		# Postgres creates these floating point numbers if an AVG aggregate function is invoked whereas Octo does not.
		mv $fname.ref $fname.ref1
		mv $fname.log $fname.log1
		# The below step converts for example : 2.5000000000000000 -> 2.5
		pat1='s/\(\.[0-9]*[1-9]\)0\{1,\}$/\1/;s/\(\.[0-9]*[1-9]\)0\{1,\}|/\1|/g;'
		sed $pat1 $fname.ref1 > $fname.ref2
		sed $pat1 $fname.log1 > $fname.log2
		# The below step converts for example : 1.0000000000000000 -> 1
		pat2='s/\.0\{1,\}$//;s/\.0\{1,\}|/|/g;'
		sed $pat2 $fname.ref2 > $fname.ref3
		sed $pat2 $fname.log2 > $fname.log3
	else
		# If $trimzeroes != "trim_trailing_zeroes", still move the files into their "ref3" and "log3"
		# versions for use later on in the process.
		mv $fname.ref $fname.ref3
		mv $fname.log $fname.log3
	fi
	# Below two sed commands add leading zeros into the Octo output in order to match PSQL's output
	# Octo Output: .333   PSQL Output: 0.333
	sed 's/|\.\([0-9]*\)/|0.\1/;' $fname.log3 > $fname.log4
	sed 's/^\.\([0-9]*\)/0.\1/;' $fname.log4 > $fname.log5
	# Check if query has ORDER BY that is not inside a subquery OR if "--sort-needed-check" is present. If so, do not sort.
	# Otherwise sort as order is not guaranteed to match between Octo and Postgres.
	# grep -c can return non-zero status in case of no match hence the need for "|| true"
	orderbyexists=$(yottadb -r removeInnerQueries $query_file | grep -ci "ORDER BY") || true
	sortneededcheck=$(grep -c "sort-needed-check" $query_file) || true
	if [[ $orderbyexists -eq 0 ]] || [[ $sortneededcheck -ne 0 ]]; then
		# ORDER BY does not exist in the query. So sort the outputs.
		mv $fname.ref3 $fname.unsorted.ref3
		mv $fname.log5 $fname.unsorted.log5
		# We have noticed that setting LC_ALL to en_US.UTF8 results in different sort output in rare cases in TQG04 subtest.
		# in the "run_query_in_octo_and_postgres_and_crosscheck()" function. "C" works fine in that case. So we choose "C"
		# just for the "sort" command.
		export LC_ALL=C
		sort $fname.unsorted.ref3 > $fname.ref3
		sort $fname.unsorted.log5 > $fname.log5
		export LC_ALL=en_US.UTF8
	fi
	# Below two seds set numerical values to only have 10 digits after the decimal point as both Octo
	# and PSQL can return varying levels of precision (12 is the least amount of decimals seen so far)
	sed -re 's/([0-9]+\.[0-9]{10})[0-9]+/\1/g' $fname.ref3 >& $fname.ref
	sed -re 's/([0-9]+\.[0-9]{10})[0-9]+/\1/g' $fname.log5 >& $fname.log
	# grep -c can return non-zero status in case of no match hence the need for "|| true"
	rowcountonlycheck=$(grep -c "rowcount-only-check" $query_file) || true
	if [[ $rowcountonlycheck -eq 0 ]]; then
		# diff result is captured to stop_rocto before issuing a test fail
		diff $fname.ref $fname.log >& $fname.diff || true
		if [[ -s $fname.diff ]]; then
			run_query_stop_rocto
			exit 1
		fi
	else
		# .sql file requests only rowcount check. No exact content check.
		# Check only # of lines in output match. Not contents of lines.
		psqllines=$(wc -l $fname.ref | awk '{print $1}')
		octolines=$(wc -l $fname.log | awk '{print $1}')
		if [[ $psqllines -ne $octolines ]]; then
			echo "Expected $psqllines lines but found $octolines lines in $fname.log"
			run_query_stop_rocto
			exit 1
		fi
	fi
}

run_query_stop_rocto() {
	# This function is only used inside the test framework
	if [[ $randclientstr != "OCTO" ]]; then
		if [[ ! -z $isnotexternal ]];then
			isnotexternal=
			test_port=
			stop_rocto
		fi
	fi
	randclientstr=
}

run_query_generator() {
  databaseName="$1"
  prefix="$2"
  usejdbcoption="$3"
  useextended="$4"
  numQueries=100	# For now generate 100 queries by default (good balance of test run time vs test coverage)

  if [[ "" != $usejdbcoption ]]; then
	numQueries=$(( $numQueries / 4 ))	# Generate 25% of queries to save time in the pipeline for JDBC tests
  fi

  cp @PROJECT_SOURCE_DIR@/tests/fixtures/QueryGenerator.m .
  load_fixture "$databaseName.sql"
  load_fixture "$databaseName.zwr"

  $ydb_dist/mumps -run QueryGenerator "$databaseName.sql" "$databaseName.zwr" "$numQueries" "$prefix"

  count=0
  queryfile=$prefix$count
  queryfile+=".sql"
  run_query_in_octo_and_postgres_and_crosscheck_multiple_queries $databaseName $queryfile "trim_trailing_zeroes" $usejdbcoption $useextended
}
