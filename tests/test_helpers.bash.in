#################################################################
#								#
# Copyright (c) 2019-2021 YottaDB LLC and/or its subsidiaries.	#
# All rights reserved.						#
#								#
#	This source code contains the intellectual property	#
#	of its copyright holder(s), and is made available	#
#	under a license.  If you do not know the terms of	#
#	the license, please stop and do not read further.	#
#								#
#################################################################

setup_bats_env() {
	export test_temp=$(mktemp -d @TEST_OUTPUT_DIR@/bats-test.XXXXXX)
	echo "Temporary files in: $test_temp"
	exec >  >(tee -ia $test_temp/stdout.txt)
	exec 2> >(tee -ia $test_temp/stderr.txt >&2)
	cd $test_temp
	# For expect tests, xterm may try to add more control characters. Avoid that by setting a minimal terminal.
	# https://gitlab.com/YottaDB/DBMS/YDBOcto/-/issues/717
	export TERM=dumb
	# Disable any user-level (~/.inputrc) customizations that can cause test failures (e.g. TERR008, TSC18 subtests)
	# For example "set editing-mode vi" in ~/.inputrc somehow re-enables tab-completion even though Octo disables it
	#	by calling `rl_bind_key('\t', rl_insert);`
	export INPUTRC=/etc/inputrc
	# Below has been seen to avoid Ctrl-M ('^M') characters in long query strings
	export COLUMNS=4096
	# Create a a log of the test being run
	echo " --> Running test [$BATS_TEST_FILENAME] : subtest [$BATS_TEST_DESCRIPTION] : in $PWD : build type @CMAKE_BUILD_TYPE@ " > bats_test.out
}

save_env_variables() {
	# Note: The below set of lines also exist in `tools/ci/build.sh` so any change here might need to be made there too.
	# Log env vars, shell vars, locale info in files for later analysis of test failures.
	# Mask any sensitive env vars out.
	env | grep -vE "HUB_USERNAME|HUB_PASSWORD|CI_JOB_TOKEN|CI_REGISTRY_PASSWORD|CI_BUILD_TOKEN|CI_REPOSITORY_URL" > dbg_env.out
	set | grep -vE "HUB_USERNAME|HUB_PASSWORD|CI_JOB_TOKEN|CI_REGISTRY_PASSWORD|CI_BUILD_TOKEN|CI_REPOSITORY_URL" > dbg_set.out
	locale > dbg_locale.out
	locale -a > dbg_localeall.out
	# This exports an environment that can be easily sourced to examine the test interactively
	declare -p -x | grep -- "-x ydb_" | grep -v ydb_unset_ | grep -v ydb_sav_ > test.env
}

init_test() {
	setup_bats_env

	# Set YDB related env vars
	unset ydb_gbldir gtmgbldir	# needed or else ydb_env_set can issue ZGBLDIRACC error (due to it calling MUPIP DUMPFHEAD)
					# if ydb_gbldir is defined and points to a non-existent gld file.
	# The below is needed to ensure that the bats tests run fine even if run concurrently.
	# Without setting ydb_dir to the current directory (which is unique for each test/subtest),
	# concurrent calls to ydb_env_set would create/update/delete journal files under ~/.yottadb/r*/g/yottadb.mjl
	# at the same time which would cause mysterious errors (e.g. %YDB-E-JNLFILEOPNERR) in random tests depending on timing.
	export ydb_dir=.
	# Pipeline jobs start with `gtmdir` set (to "/data") so fix that to be in sync with `ydb_dir`
	# or else `ydb_env_set` would issue a %YDBENV-F-YDBGTMMISMATCH error.
	export gtmdir=$ydb_dir
	# Randomly choose whether to use UTF-8 or M mode
	source @YOTTADB_INCLUDE_DIRS@/ydb_env_unset
	if [[ $(( $RANDOM % 2)) -eq 0 ]]; then
		export ydb_chset=UTF-8
		utf8_path="/utf8"
	else
		export ydb_chset=M
		utf8_path=""
		# Set ydb_icu_version to the right value even if we chose M mode. This way, wherever we set ydb_chset to "UTF-8"
		# (e.g. in "load_fixture()" function while loading the northwind.zwr fixture) things will work fine.
		# Not setting this can cause ICUSYMNOTFOUND errors. No need to do this for the case when "ydb_chset" is chosen
		# to be "/utf8" as the "ydb_env_set" call done after this if/else block will set that env var.
		export ydb_icu_version=`pkg-config --modversion icu-io`
	fi
	source @YOTTADB_INCLUDE_DIRS@/ydb_env_set
	# Run tests locally without installing Octo/Rocto to $ydb_dist
	if [[ @DISABLE_INSTALL@ == "ON" ]]; then
		export PATH="@PROJECT_BINARY_DIR@/src:$PATH"
		# tests/fixtures has lots of M programs. Make sure .o files for those get created in current test output directory
		# (and not in tests/fixtures) as it can otherwise cause .o file format issues (e.g. if YDB changes the M .o file format)
		# since tests/fixtures persists a lot longer than the test output directory. We had previously seen COLLATIONUNDEF errors
		# while trying to use the master branch of YDB repo when .o files created by r1.29 in Nov 2019 were tried to be used
		# by r1.29 in Dec 2019 (after the .o file format was changed in between). Having the .o files in the test output directory
		# avoids such issues. This comment also applies to the similar line in the below `else` block.
		ydbroutines=".(. @PROJECT_SOURCE_DIR@/tests/fixtures) @PROJECT_BINARY_DIR@/src$utf8_path/_ydbocto.so @PROJECT_BINARY_DIR@"
	else
		# Add the UTF-8 path, if set, as this is not done by build.sh in the pipeline
		ydbroutines=".(. @PROJECT_SOURCE_DIR@/tests/fixtures)"
		ydbroutines="$ydbroutines @PROJECT_BINARY_DIR@"
	fi
	export ydb_routines="$ydbroutines $ydb_routines"

	# Set env var to allow .m and .o file timestamps to be identical. This is expected to be particularly useful
	# in the pipeline test runs where we have seen the _ydboctoP*.o file almost always created with a timestamp
	# that is 1 second later than the corresponding _ydboctoP*.m file. We suspect this is due to the underlying
	# file system granularity being at 1-second (instead of 1-milli/micro/nano second). Once this env var is
	# set, YDB will not wait for the .o file time stamp to be different than the .m file. Since there are almost
	# thousands of queries that run in the pipeline tests lots of these 1-second delays adds up to minutes of
	# slowdown which should all hopefully go away with this env var set.
	export ydb_recompile_newer_src=TRUE
	# Below is needed to avoid wildcard expansion order (e.g. cat *.m used in various tests) changing based on LC_TYPE
	# It is also needed to avoid sort order changing (and reference file issues) based on LC_CTYPE being "C" or "en_US.UTF8".
	# Fixing LC_ALL to a particular value ensures caller environment does not affect test results.
	export LC_ALL=en_US.UTF8
	# Avoid generating .pyc files during tests, which clutter up the source directory
	# All the python scripts take a very small fraction of the test time,
	# so this does not impact pipeline times in any significant way.
	export PYTHONDONTWRITEBYTECODE=1
}

init_tls() {
	# Generate CA key and certificate
	openssl genpkey -algorithm RSA -pkeyopt rsa_keygen_bits:2048 -pass pass:tester -out $test_temp/CA.key
	openssl req -new -nodes -key $test_temp/CA.key -passin pass:tester -days 365 -x509 \
			-subj "/C=US/ST=PA/L=Malvern/O=Octo/CN=www.yottadb.com" -out $test_temp/CA.crt
	# Create server key and certificate request
	openssl genpkey -algorithm RSA -pkeyopt rsa_keygen_bits:2048 -pass pass:tester -out $test_temp/server.key
	openssl req -new -key $test_temp/server.key -passin pass:tester \
		-subj "/C=US/ST=PA/L=Malvern/O=Octo/CN=www.yottadb.com" -out $test_temp/server.csr
	# Sign certificate based on request and local CA
	openssl x509 -req -in $test_temp/server.csr -CA $test_temp/CA.crt -CAkey $test_temp/CA.key -CAcreateserial \
		-out server.crt -days 365
	# Pass private key password to environment variable
	echo tester | $ydb_dist/plugin/gtmcrypt/maskpass | cut -f 3 -d " " >> env.log
	export ydb_tls_passwd_OCTOSERVER=$(echo tester | $ydb_dist/plugin/gtmcrypt/maskpass | cut -f 3 -d " ")
	export ydb_crypt_config=$test_temp/octo.conf
	cat <<OCTO &>> $test_temp/octo.conf
rocto: {
	ssl_on: true;
}

tls: {
	CAfile: "$test_temp/CA.crt";
	CApath: "$test_temp/";
	OCTOSERVER: {
		format: "PEM";
		cert: "$test_temp/server.crt";
		key: "$test_temp/server.key";
	}
}
OCTO
}

copy_test_files() {
	for f in $@; do
		mkdir -p $test_temp/$(dirname $f)
		cp @PROJECT_SOURCE_DIR@/tests/$f $test_temp/$f
	done
}

# load_fixture <fixture name, relative to tests/fixtures
load_fixture() {
	fixture_name=$1
	echo "Loading fixture $fixture_name"
	if [[ $fixture_name == *.zwr ]]; then
		# Determine the chset (M or UTF-8) to use from the extract file. It will be either "UTF-8" or "".
		chset=`grep "MUPIP EXTRACT" @PROJECT_SOURCE_DIR@/tests/fixtures/$fixture_name | awk '{print $4}'`
		# Keep LC_ALL as en_US.UTF8 (set in "init_test()" function) for both M and UTF-8 mode. That should work just fine.
		ydb_chset=$chset $ydb_dist/mupip load @PROJECT_SOURCE_DIR@/tests/fixtures/$fixture_name
	elif [[ $fixture_name == *.sql ]]; then
		if [[ $2 == "subtest" ]]; then
			cp @PROJECT_SOURCE_DIR@/tests/fixtures/$fixture_name $fixture_name
			grep -v '^#' $fixture_name 2>&1 | tee -a output.txt	   # Filter out copyright from output
			grep -vE '^-- |^#' $fixture_name > input.sql || [ $? = 1 ] # Filter out comment lines as they otherwise clutter the parse error output (if any)
			if [[ $3 == "novv" ]]; then
				octoflags=""
			elif [[ $3 =~ "v" ]]; then	# Allow verbosity level to be passed directly
				octoflags="-$3"
			else
				octoflags="-vv"
			fi
			octo $octoflags -f input.sql 2>&1 | tee -a output.txt
		else
			octo -f @PROJECT_SOURCE_DIR@/tests/fixtures/$fixture_name
		fi
	else
		echo "Unrecognized file extension: $fixture_name"
		exit 1
	fi
}

create_postgres_database() {
	psql postgres <<PSQL &> $1.setup.out
	-- Need to use LC_COLLATE='C' to ensure string comparison of 'Z' < 'a' returns TRUE (default is en_US.UTF8)
	-- Need template=template0 to avoid the following error
	-- "new collation (C) is incompatible with the collation of the template database (en_US.UTF-8)"
	create database $1 LC_COLLATE='C' template=template0;
PSQL
	# Create a PostgreSQL role for the user running the tests and grant that user sufficient PostgreSQL permissions to
	# log in to the specified database and access all the tables there. Note that ALTER is used for the case when the user
	# already exists in PostgreSQL, but doesn't have the required settings.
	psql $1 <<PSQL 2>&1 | tee -a $1.setup.out
	CREATE ROLE $USER;
	ALTER ROLE $USER LOGIN;
	ALTER ROLE $USER PASSWORD 'ydbrocks';
	GRANT CONNECT ON DATABASE $1 TO $USER;
	GRANT ALL PRIVILEGES ON ALL TABLES IN SCHEMA public TO $USER;
PSQL
}

load_postgres_fixture() {
	# Check if $1 is a valid Postgres database name first. Do this separately as the next step has a "|| true"
	# to account for ON_ERROR_STOP and we do not want errors from a non-existent table name to be swallowed there.
	# See https://stackoverflow.com/a/16783253 for more details on the below.
	# If a database with name $1 does not exist, the below step would set $? to 1 and bats would exit right here with an error.
	psql -lqt | cut -d \| -f 1 | grep -qw $1
	# Now that we know $1 is a valid table, run the file $2 that creates tables and loads data for the database $1.
	# If the table already exists, we want to do nothing here.
	# The way we achieve this is by setting "ON_ERROR_STOP" to "on" before doing the "CREATE TABLE" inside the $2 file.
	# This way the "CREATE TABLE" will fail if the table already exists and we will skip the INSERT commands.
	# An alternative way to avoid the "ON_ERROR_STOP" is to "DROP TABLE IF EXISTS" first and then run the "CREATE TABLE".
	# But this has the issue that any table privileges that were added for multiple users will get removed and have to be
	# added again. Additionally, if tests are run concurrently, it is possible a test using this Postgres table run
	# at the same time the table is being dropped/created leading to mysterious test failures due to a missing/incomplete
	# table. Hence we stick with the "ON_ERROR_STOP" approach. Since this can return a non-zero exit status and that is
	# expected, we use the "|| true" below to avoid the bats framework from treating this as a failure and stopping the test.
	psql $1 -c '\set ON_ERROR_STOP on' -f @PROJECT_SOURCE_DIR@/tests/fixtures/$2 &> $1.psql.load || true
}

createdb() {
	# Need to have non-default key size specified as otherwise it won't fit
	# VistA globals.  Technically, VistA only needs the M standard's
	# maximum of 256 whereas the default maximum in GDE is 64. But the
	# maximum possible key size of 1019 does not hurt.
	export ydb_gbldir="yottadb.gld"
	echo "ydb_gbldir: $ydb_gbldir"
	# Create two regions by default. One to hold application/user data. One to hold Octo internal data (%ydbocto* namespace)
	# But if user specified number of regions explicitly (only allowed value currently is 1) then create 1 region only.
	if [[ $1 == "1" ]]; then
		$ydb_dist/yottadb -run ^GDE <<FILE
		change -r DEFAULT -key_size=1019 -record_size=1048576
		change -segment DEFAULT -file_name=$test_temp/mumps.dat
		change -r DEFAULT -NULL_SUBSCRIPTS=true
		exit
FILE
	else
		$ydb_dist/yottadb -run ^GDE <<FILE
change -region DEFAULT -null_subscripts=true -record_size=1048576 -key_size=1019
change -segment DEFAULT -file_name=$test_temp/mumps.dat
add -name %ydbocto* -region=OCTOREG
add -region OCTOREG -dyn=OCTOSEG
add -segment OCTOSEG -file=$test_temp/octo.dat
change -region OCTOREG -null_subscripts=true -key_size=1019 -record_size=1048576
exit
FILE
	fi
	rm *.dat || true
	$ydb_dist/mupip create
	# Set the below env var to ensure no DBFILEXT messages show up in syslog and in turn in individual test output files
	# e.g. if a test redirects stderr to a file output.txt, then syslog messages would show up in that file if run through
	# the pipeline causing a test failure if that is compared against a reference file (e.g. TC001 in test_create_table.bats.in)
	export ydb_dbfilext_syslog_disable=1

	save_env_variables

}

gde_add_region() {
	$ydb_dist/yottadb -run GDE <<FILE
add -name $2 -region=$1
add -region $1 -dynamic=$1 -key_size=1019 -record_size=1048576
add -segment $1 -file_name=$test_temp/$1.dat
exit
FILE
	$ydb_dist/mupip create -region=$1
}

gde_add_name() {
	echo "NAME: $1"
	$ydb_dist/yottadb -run GDE <<FILE
add -name $1 -region=$2
exit
FILE
}

set_null_subs() {
	$ydb_dist/mupip set -region $1 -NULL_SUBSCRIPTS=$2
}

find_open_port() {
	local start_port=$1
	# We need to find an open port on the system.
	# Note that it is possible tests are run concurrently. In that case,
	# this function can be invoked at the same time from different processes.
	# Therefore we need to get a lock and execute the below code.

	local lockfile=@TEST_OUTPUT_DIR@/find_open_port
	# Find a free file descriptor in the current shell process first (using {var} syntax in bash)
	exec {lockfd}>$lockfile.lock
	# At this point "$lockfd" contains the value of the fd in the current shell process
	# which was obtained as a result of opening the file "$lockfile.lock"

	# Now lock this file descriptor to effectively lock the file.
	flock $lockfd

	# ---------------- BEGIN : Code that runs under the exclusive lock -----------------

	# Need to switch to portno allocation .gld/.dat files for below commands (to find an open port) hence the "ydb_gbldir="
	# env var set just before each command invocation below. This avoids having to store the env var "ydb_gbldir" value (set
	# by createdb()) in a local variable, switch the env var to the portno allocation .gld and then switch it back to the
	# saved value at the end of these commands.

	# If not already done, create database file that stores portno allocations
	if ! [ -e $lockfile.gld ]; then
		ydb_gbldir=$lockfile.gld $ydb_dist/yottadb -run GDE "change -segment DEFAULT -file_name=$lockfile.dat"
		ydb_gbldir=$lockfile.gld $ydb_dist/mupip create
	fi

	# Find open port
	open_port=`ydb_gbldir=$lockfile.gld $ydb_dist/yottadb -run findopenport^portno $start_port`

	echo $open_port
	# Record free port number that was found and time when it was found for later debugging in case it is needed
	echo " `date -Ins` : Allocated port [$open_port]" >> portno.out

	# ---------------- END  : Code that runs under the exclusive lock -----------------

	# Drop lock on the file
	flock -u $lockfd
}

run_octo_allow_error() {
	if [[ ! -f $1 ]]; then
		# If the input file isn't present in the current directory, it's a fixture.
		# So, strip the comments and copy it into the working directory for use below.
		strip_sql_comments $1
	fi
	# Run octo, but don't fail the test when an error is encountered. Used for testing error cases.
	octo $3 -f $1 >> $2 2>&1 || true
}

start_rocto() {
	create_user $USER "ydbrocks" &> /dev/null
	echo "ROCTO START" >> env.log
	if [[ $2 == "quiet" ]]; then
		verbosity="-v"
		opt=$3
	elif [[ $2 == "verbose" ]]; then
		verbosity="-vvv"
		opt=$3
	else
		verbosity="-vv"
		# Allow optional second argument
		opt=$2
	fi
	rocto_port=$(find_open_port $1)
	rocto $verbosity -p $rocto_port $opt &> rocto.log &
	rocto_pid=$!
	echo $rocto_pid > rocto.pid
	# Store rocto port number in the file "rocto.port" for later use in "stop_rocto"
	echo $rocto_port > rocto.port
	while [[ ! -e rocto.log || "$(grep -c "rocto started" rocto.log)" == "0" ]]; do
		sleep .1s
		# If the rocto job disappears while checking, signal failure.
		if [[ ! -e /proc/$rocto_pid ]]; then
			return -1
		fi
	done
	echo $rocto_port
}

stop_rocto() {
	# Wait for rocto listener to die
	if [[ -e rocto.pid ]]; then
		listener_pid=$(cat rocto.pid)
		# Wait for rocto servers (i.e. children of the listener) to die
		ps --no-headers --ppid $listener_pid | awk '{print $1}' | while read server_pid; do
			# Wait for rocto server process to actually die
			tail --pid=$server_pid -f /dev/null
		done
		$ydb_dist/mupip stop $listener_pid
		# Wait for rocto process to actually die
		tail --pid=$listener_pid -f /dev/null
		# Release port number held by rocto (stored in the file "rocto.port")
		local lockfile=@TEST_OUTPUT_DIR@/find_open_port
		local test_port=`cat rocto.port`
		ydb_gbldir=$lockfile.gld $ydb_dist/yottadb -run releaseport^portno $test_port
		# Record port number release time for later debugging in case it is needed
		echo " `date -Ins` : Released port [$test_port]" >> portno.out
	fi
}

run_psql() {
	if [[ $2 =~ ".sql" ]]; then
		# Load a fixture
		strip_sql_comments $2
		PGPASSWORD=ydbrocks psql $3 -U ydb "host=localhost port=$1" < $2
	else
		PGPASSWORD=ydbrocks psql $3 -U ydb "host=localhost port=$1"
	fi
}

run_psql_auth() {
	if [[ @YDB_TLS_AVAILABLE@ -eq 1 ]]; then
		PGPASSWORD=$2 psql --no-align -U $1 "host=localhost port=$3"
	else
		PGPASSWORD=$2 psql --no-align -U $1 "sslmode=disable host=localhost port=$3"
	fi
}

run_psql_expect() {
	unset PGPASSWORD
	(expect -d -f @PROJECT_SOURCE_DIR@/tests/fixtures/$1.exp $2 > expect.out) &> expect.dbg
}

create_user() {
	echo -en "$2\n$2" | yottadb -r %ydboctoAdmin add user $1
}

delete_users() {
	for user in $@; do
		yottadb -r %ydboctoAdmin delete user $user
	done
}

setup_go() {
	mkdir -p "$test_temp/go/src/"
	export GOPATH="$test_temp/go"
}

run_go() {
	cp -r @PROJECT_SOURCE_DIR@/tests/go/src/$1 $GOPATH/src/$1
	# As of Go 1.16, the default behavior is GO111MODULE=on but we want to keep using the old GOPATH way,
	# therefore set the below env var to Go not to use the Go Modules feature. This is not needed by
	# Go versions older the Go 1.16 but it does not hurt.
	export GO111MODULE=off
	go get $1
	go build $1
	$GOPATH/bin/$1 $2
	# Executables created by go occupy a noticeable part of the artifacts.zip file created in the pipeline.
	# They are not considered necessary at this point for test failure analysis so remove them.
	rm -f $1
}

run_java() {
	# Check if .java file has already been compiled to a .class file. If so skip that step.
	if [[ ! -e @PROJECT_BINARY_DIR@/$1.class ||
		$(stat -c '%Y' @PROJECT_BINARY_DIR@/$1.class) -le $(stat -c '%Y' @PROJECT_SOURCE_DIR@/tests/fixtures/$1.java) ]]; then
		# Copy .java file from fixtures directory to current directory to create .class file
		cp @PROJECT_SOURCE_DIR@/tests/fixtures/$1.java @PROJECT_BINARY_DIR@
		# Compile the .java file to a .class file
		javac @PROJECT_BINARY_DIR@/$1.java	# will create the file $1.class

	fi
	strip_sql_comments $3
	if [[ $1 == "run_query" ]]; then
		# Only append ".sql" for queries derived from fixtures, not for generated queries
		if [[ -f $3 ]]; then
			query=$(cat $3)
		else
			query=$(cat $3.sql)
		fi
	elif [[ $1 == "run_multi_query" ]]; then
		query=$3.sql
	elif [[ $1 == "run_multiple_query_files" ]]; then
		query=$3
	else
		query=""
	fi
	psql_jar="@PROJECT_BINARY_DIR@/postgresql-$JDBC_VERSION.jar"
	if ! [[ -v JDBC_VERSION ]]; then
		echo "error: \$JDBC_VERSION was required but not set. Consult the Octo README for more information."
	elif ! [[ -e "$psql_jar" ]]; then
		echo "error: $psql_jar does not exist. Cannot continue."
	fi
	# Run the $1.java file now that the $1.class file has been compiled/created.
	# Ensure both the postgresql-$JDBC_VERSION.jar file and the $1.class file will be found in the -classpath specification below
	java -classpath @PROJECT_BINARY_DIR@/postgresql-$JDBC_VERSION.jar:@PROJECT_BINARY_DIR@ $1 $2 "$query" $4 $5 $6
}

strip_psql_header() {
	sed -i '/Password for user /,/^[[:space:]]*$/d' $1
}

strip_expect_artifacts() {
	sed -i '/^.*spawn \/bin\/bash.*$/d' $1
	sed -i '/^.*stty cols 4096.*$/d' $1
	sed -i '/^.*psql -U ydb -h localhost -p .*$/d' $1
	# Shell prompt would have the bats-test.* directory name in some platforms (e.g. CentOS).
	# Shell prompt would have the directory name as $PWD in some platforms (e.g. Ubuntu)
	# Account for that in the 2 lines below. The [#$] is to account for either a # or a $ as the shell prompt
	sed -i '/^.*bats-test\..*[#$].*$/d' $1
	sed -i '/^.*\$PWD[#$]*.*$/d' $1
}

# Filter out '#' comment lines and blank lines, if any
strip_sql_comments() {
	# Remove .sql extension (occurs when root caller is run_query_in_octo_and_postgres_and_crosscheck)
	filename=$(echo $1 | cut -d'.' -f 1)
	# Comments are only present for fixtures
	if [[ -f @PROJECT_SOURCE_DIR@/tests/fixtures/$filename.sql ]]; then
		cp @PROJECT_SOURCE_DIR@/tests/fixtures/$filename.sql $filename.in
		grep -v '^#' $filename.in | grep -v '^[[:space:]]*$' > $filename.sql
	fi
}

verify_plans_xrefs_gvns_triggers() {
	queryoutputfile=$1
	referencefile=$2
	mupipextractfile=$3
	# Note that since this framework function is called from various tests (with different scenarios),
	# it is possible that some steps below have NO output. In that case we do not want the bats to fail.
	# Hence the "|| true" usage below.
	echo '# Verify generated _ydbocto*.m and _ydbocto*.o plan files' >> $referencefile
	ls -1 _ydbocto*.{m,o} >> $referencefile || true
	echo "# Verify how many queries created new plans" >> $referencefile
	grep "execute SQL query" $queryoutputfile >> $referencefile || true
	$ydb_dist/mupip extract $mupipextractfile
	echo "# Verify db nodes correctly represent tables and plans (including xref plans) that rely on them" >> $referencefile
	grep -w "tableplans" $mupipextractfile | grep -vw output_columns >> $referencefile || true
	echo "# Verify db nodes correctly represent functions and plans that rely on them" >> $referencefile
	grep -w "plan_metadata" $mupipextractfile | grep -w "functions" >> $referencefile || true
	echo "# Verify db nodes for plans that are reusable (i.e. do not need to be regenerated)" >> $referencefile
	grep -w "plan_metadata.*output_key" $mupipextractfile >> $referencefile || true
	echo "# Verify db nodes correctly represent cross-references" >> $referencefile
	grep -E "xref_status|ydboctoxref" $mupipextractfile >> $referencefile || true
	echo "# Verify db nodes correctly represent src/obj dirs of generated plans (including xref plans)" >> $referencefile
	grep "plandirs" $mupipextractfile >> $referencefile || true
	echo "# Verify currently installed triggers" >> $referencefile
	# MUPIP TRIGGER -SELECT currently has an additional output line containing "Output File:" before the actual
	# list of triggers (if any are installed). This can be confusing in reference files so remove this.
	mupip trigger -select < /dev/null | grep -v "Output File:" >> $referencefile || true
	echo "" >> $referencefile
}

# Filter file path (either relative or absolute)
# Assumes `clean_output.txt` exists and modifies it in place.
# Modifies the given file in place.
filter_file_path() {
	sed -i 's# [/\.][^ ]*/.*\.[yc]:[0-9]*# PATH:LINENUM#' clean_output.txt
}

verify_output() {
	# We had cases where loading octo-seed.sql in "createdb()" issued a "ERR_CANNOT_CREATE_FUNCTION" error but
	# the test passed inspite of that because that output went to stderr.txt whereas the test only looked at output.txt.
	# Therefore, since this is a central function that is usually invoked at the end of a subtest, take this opportunity
	# to verify stderr.txt do not have any Octo errors in them (prefix of [ERROR]). Fail the test if they do.
	[[ $(grep "\[ERROR\] " stderr.txt | wc -l) -eq 0 ]]

	echo "Comparing outref/$1.ref $2"
	copy_test_files outref/$1.ref
	cp $2 clean_output.txt

	# Universal filters
	# Filter time and dates
	sed -i 's/[0-9]\{4\}-[0-9]\{2\}-[0-9]\{2\}/DATE/g' clean_output.txt
	sed -i 's/[0-9]\{2\}:[0-9]\{2\}:[0-9]\{2\}/TIME/g' clean_output.txt
	filter_file_path
	# Filter rule #s and line #s printed by flex in TRACE verbosity level output
	sed -i 's/Entering state [0-9]*/Entering state sss/' clean_output.txt
	# The below line output format changed between bison 3.5.1 (Ubuntu 20.04) and bison 3.7 (Ubuntu 20.10)
	# Therefore filter it out of the reference file to avoid complications there.
	sed -i '/Stack now [0-9 ]*/d' clean_output.txt
	# Filter out "Reading a token" as it shows up differently in different bison versions
	sed -i 's/Reading a token: //g' clean_output.txt	# bison 3.5.1 format
	sed -i '/^Reading a token$/d' clean_output.txt		# bison 3.7.1 format
	sed -i 's/\(Reducing stack by rule\) \([0-9]*\) \((line\) [0-9]*)/\1 rrr \3 lll)/' clean_output.txt
	# Filter octo.conf from $ydb_dist and $HOME, i.e. not in the local directory
	# Cannot specify $ydb_dist directly as it varies depending on UTF-8 mode selection,
	# so just use "plugin/octo" as that is guaranteed to be in the $ydb_dist path.
	# Note also that the $ydb_dist case is unconditionally removed - otherwise this
	# message will cause outref discrepancies for the two DISABLE_INSTALL cases ("ON" and "OFF").
	sed -i "/\/.*\/plugin\/octo\/octo.conf/d" clean_output.txt
	sed -i "s,$HOME\/octo.conf,\$HOME\/octo.conf," clean_output.txt
	# Filter M routine name
	sed -i 's/_ydboctoP[A-Za-z0-9]*\.m/_ydboctoP*.m/g' clean_output.txt
	sed -i 's/_ydboctoX[A-Za-z0-9]*\.m/_ydboctoX*.m/g' clean_output.txt
	# Filter M routine name with %
	sed -i 's/\%ydboctoP[a-zA-Z0-9]*,/%ydboctoP*,/' clean_output.txt
	# Filter OBJ file name
	sed -i 's/_ydboctoP[A-Za-z0-9]*\.o/_ydboctoP*.o/g' clean_output.txt
	sed -i 's/_ydboctoX[A-Za-z0-9]*\.o/_ydboctoX*.o/g' clean_output.txt
	# Filter cursor number
	sed -i 's/cursor [0-9]*/CURSOR_NUM/' clean_output.txt
	# Filter ydb_* environment variables
	sed -i '/^.*\[ INFO\] PATH:LINENUM DATE TIME : INFO_ENV_VAR : # .*$/d' clean_output.txt
	# Convert non-deterministic $PWD into a deterministic one
	# Convert symbolic links into absolute paths
	cd -P $PWD
	sed -i 's,'$PWD',$PWD,g' clean_output.txt
	# Convert PROJECT_SOURCE_DIR references to generic one
	sed -i 's,'@PROJECT_SOURCE_DIR@',SRCDIR,g' clean_output.txt
	# Convert YDB C source file/line references to generic one
	sed -i 's,\(called from module\) .*/\(sr_.*\) at line [0-9]*,\1 \2 at line LINE,' clean_output.txt
	# Filter version number
	sed -i 's/Octo version [0-9]\.[0-9]\.[0-9]/Octo version x\.x\.x/' clean_output.txt
	sed -i 's/Rocto version [0-9]\.[0-9]\.[0-9]/Rocto version x\.x\.x/' clean_output.txt
	# Filter git commit hash
	sed -i 's/Git commit: [0-9,a-f]*/Git commit: xxxx/' clean_output.txt
	# Filter git uncommitted changes
	sed -i 's/Uncommitted changes: .*/Uncommitted changes: false/' clean_output.txt
	# Filter forked process pid
	sed -i 's/rocto server process forked with pid [0-9]*/rocto server process forked with pid PID/' clean_output.txt
	# Convert psql prompt to root for compatibility when running tests on systems with different PostgreSQL user permissions
	# Specifically, some test machines do not run tests with full PostgreSQL user permissions and so receive a prompt with '>' instead of '#'
	sed -i 's/=>/=#/' clean_output.txt

	# Selectively process output
	for i in "$@"; do
		# Filter verbosity statements from octo and psql
		# a default octo install will have WARNING verbosity so INFO and DEBUG will need to be filtered for most tests
		if [[ $i == "noinfo" ]]; then
			sed -i '/\[ INFO\]\|^INFO:/d' clean_output.txt
		fi
		if [[ $i == "nodebug" ]]; then
			sed -i '/\[DEBUG\]\|^DEBUG:/,/^[[:space:]]*$/d' clean_output.txt
		fi
		if [[ $i == "nosenderror" ]]; then
			sed -i '/failed to send message/d' clean_output.txt
		fi
		if [[ $i == "noconnclose" ]]; then
			# Remove "connection closed cleanly" messages
			sed -i '/connection closed cleanly/d' clean_output.txt
		fi
		if [[ $i == "noprompt" ]]; then
			# Remove octo prompt, but keep remainder of line
			sed -i 's/^.*OCTO>//' clean_output.txt
		fi
		if [[ $i == "nopromptline" ]]; then
			# Remove octo prompt lines
			sed -i '/OCTO>/,$!d' clean_output.txt
		fi
		if [[ $i == "psql" ]]; then
			# filter socket info
			sed -i 's/\[[0-9]*\.[0-9]*\.[0-9]*\.[0-9]*:[0-9]*\]/\[SOCKET\]/' clean_output.txt
		fi
		if [[ $i == "noport" ]]; then
			# filter port from INFO messages in rocto
			sed -i 's/rocto started on port [0-9]*/\[PORT\]/' clean_output.txt
		fi
		if [[ $i == "noexpect" ]]; then
			# Remove expect script artifacts
			strip_expect_artifacts clean_output.txt
			strip_psql_header clean_output.txt
		fi
		if [[ $i == "stripreturns" ]]; then
			sed -i 's/\r//g' clean_output.txt
		fi
		if [[ $i == "striphex" ]]; then
			# Remove any hex values consisting of 2 to 8 hex characters (16-64 bits)
			sed -i 's/0x[0-9a-fA-F]\{2,16\}/HEX/' clean_output.txt
		fi
		if [[ $i == "noforcedhalt" ]]; then
			sed -i '/%YDB-F-FORCEDHALT/d' clean_output.txt
		fi
		if [[ $i == "noconfig" ]]; then
			# Remove Octo's "Loading config from ..." message while leaving other INFO messages intact
			sed -i '/Loading config from/d' clean_output.txt
		fi
		# The sort argument should be passed last to allow sorting of final output file
		if [[ $i == "sort" ]]; then
			mv clean_output.txt clean_output.txt.nosort
			cat clean_output.txt.nosort | sort_LC_ALL_C >& clean_output.txt
		fi
	done
	escape_sequence_sanitize
	# test_where_in.bats TWI12 when executed with JDBC driver fails without the following code.
	# This is because the semicolon at the end of a query is left out of the physical plan in this case.
	# To avoid such failure we remove all occurrences of semicolons before the comparison.
	local isjdbcclient=$(grep "randclient:JDBC" dbg_env.out)
	if [[ ! -z "$isjdbcclient" ]]; then
		sed 's/;//g' outref/$1.ref > jdbc_$1.ref
		sed 's/;//g' clean_output.txt > jdbc_clean_output.txt
		diff jdbc_$1.ref jdbc_clean_output.txt
	else
		diff outref/$1.ref clean_output.txt
	fi
	return
}

escape_sequence_sanitize() {
	# On RHEL7 and CentOS7, the OCTO> prompt has some escape sequences before it (due likely to an older version of readline()).
	# Need that trimmed out as it will cause reference file issues.
	if [ ! -x "$(which rpm)" ] || ! grep -q 'VERSION_ID="7' /etc/os-release; then
		export disable_escape_sequence_sanitize=1
	fi
	if [[ -z "$disable_escape_sequence_sanitize" ]]; then
		mv clean_output.txt clean_output.txt.nosanitize
		perl @PROJECT_SOURCE_DIR@/tests/fixtures/escape_sequence_sanitize.pl clean_output.txt.nosanitize > clean_output.txt
	fi
}

count_for_loops() {
	numloops=$(grep -c "FOR " $1) || true	# grep -c can return non-zero status in case of no match hence the need for "|| true"
	if [[ $numloops -ne $2 ]]; then
		echo "Expected $2 FOR loops but found $numloops loops instead in $1"
		return -1
	fi
	return
}

count_num_occurrences() {
	numoccurrences=$(grep -c "$1" $2) || true # grep -c can return non-zero status in case of no match hence the need for "|| true"
	if [[ $numoccurrences -ne $3 ]]; then
		echo "Expected $3 occurrences of [$1] but found $numoccurrences occurrences instead in $2"
		return -1
	fi
	return
}

run_query_in_octo_and_postgres_and_crosscheck_multiple_queries() {
	# Query file has multiple queries.
	# Split it into multiple query files each containing one query and invoke "run_query_in_octo_and_postgres_and_crosscheck()".
	# Spliting queries are needed as otherwise Postgres and Octo output cannot be easily compared.

	# Init
	database="$1"
	queryfile="$2"
	psql_port="5432"
	local num_clients
	# psql errors will never have the same output as octo errors, so they cannot be used in crosscheck.
	# Therefore any error should fail the test.
	# However, on CentOS 7 there are many spurious failures due to postgres not being able to infer the type of NULL.
	# Ignore these failures because they do not affect the correctness of Octo.
	# Postgres versions under 10 throw "failed to find conversion function from unknown to text" error
	# when queries such as ".. WHERE a.firstName = (SELECT NULL);" access postgres through JDBC driver.
	# To avoid such failures the JDBC client is excluded in such a case.
	postgres_version="$(psql --command='show server_version;' --no-align --tuples-only postgres)"
	local psql_cmd
	if [ "$(echo "$postgres_version" | cut -d . -f 1)" -lt 10 ]; then
		psql_cmd=psql
		num_clients=2
	else
		psql_cmd="psql -v ON_ERROR_STOP=1"
		num_clients=3
	fi
	# Process parameters
	for i in "$@"; do
		if [[ $i == "trim_trailing_zeroes" ]]; then
			trimzeroes="trim_trailing_zeroes"
		fi
		if [[ $i == "usejdbc" ]]; then
			usejdbc="usejdbc"
		fi
		if [[ $i == "useextended" ]]; then
			jdbcprotocol="useextended"
		fi
		if [[ $i == "useocto" ]]; then
			randclientstr="OCTO"
		fi
	done
	if [[ ($usejdbc == "usejdbc") && ("$jdbcprotocol" == "") ]]; then
		# Randomly choose simple or extended query protocol
		jdbcprotocol=$(( $RANDOM % 2))
		if [[ $jdbcprotocol == 0 ]]; then
			jdbcprotocol=usesimple
		else
			jdbcprotocol=useextended
		fi
	fi
	if [[ $randclientstr != "OCTO" && $usejdbc != "usejdbc" ]]; then
		# Randomly select client
		randclient=$(( $RANDOM % $num_clients))
		# Setup client parameters based on the selection
		if [[ $randclient == 2 ]]; then
			randclientstr="JDBC"
			jdbcprotocol=$(( $RANDOM % 2))
			if [[ $jdbcprotocol == 0 ]]; then
				jdbcprotocol=usesimple
			else
				jdbcprotocol=useextended
			fi
			if [[ -z "$test_port" ]]; then
				# test_port not set in subtest, set isnotexternal to
				# inform further script to stop_rocto on test completion or failure
				isnotexternal=1
				test_port=$(start_rocto 1344)
			fi
		elif [[ $randclient == 1 ]]; then
			randclientstr="PSQL"
			load_fixture default_user.zwr
			if [[ -z "$test_port" ]]; then
				# test_port not set in subtest, set isnotexternal to
				# inform further script to stop_rocto on test completion or failure
				isnotexternal=1
				test_port=$(start_rocto 1339)
			fi
		else
			randclientstr="OCTO"
		fi
	fi
	echo "Cross check parameters:" >> dbg_env.out
	echo "postgres_version:$postgres_version randclient:$randclientstr usejdbc:$usejdbc jdbcprotocol:$jdbcprotocol" >> dbg_env.out

	if [[ ! -f $queryfile ]]; then
		if [[ ! -f @PROJECT_SOURCE_DIR@/tests/fixtures/$queryfile ]]; then
			echo "Query file: $queryfile doesn't exist"
			exit 1
		fi
		cp @PROJECT_SOURCE_DIR@/tests/fixtures/$queryfile .
	fi
	fname=`echo $queryfile | sed 's/\..*//g'`
	# Prepare query file/files for execution
	if [[ $randclientstr == "JDBC" || $usejdbc == "usejdbc" ]]; then
		# Remove commented instructions like -- sort-needed-check or -- rowcount-only-check
		# and commented queries as they are not handled well by JDBC client
		sed 's/\s*--.*//' $queryfile &> "$fname"_nocomment.sql
		sed 's/^\s*--.*//' $queryfile &> "$fname"_comment.sql
		# Maintaining two copies of the query file with one having comments is required to facilitate
		# result comparison in run_query_verify.
		@PROJECT_SOURCE_DIR@/tests/fixtures/sqllogic/split_queries.py "$fname"_comment.sql
		@PROJECT_SOURCE_DIR@/tests/fixtures/sqllogic/split_queries.py "$fname"_nocomment.sql
		# Only $fname is passed as it acts as a search pattern for run_multiple_query_files
		# to find all query files.
		run_query_in_octo_and_postgres_and_crosscheck "$fname"
	else
		@PROJECT_SOURCE_DIR@/tests/fixtures/sqllogic/split_queries.py $queryfile
		for splitfile in $fname-*.sql
		do
			run_query_in_octo_and_postgres_and_crosscheck $splitfile
		done

	fi

	# Stop Rocto when started by the test framework in parameter processing phase
	run_query_stop_rocto
	return
}

run_query_in_octo_and_postgres_and_crosscheck() {
	if [[ $usejdbc == "usejdbc" || $randclientstr == "JDBC" ]]; then
		local fname="$1"
		run_java run_multiple_query_files $psql_port "$fname"_nocomment- "$jdbcprotocol" ".psql.out" $database
		run_java run_multiple_query_files $test_port "$fname"_nocomment- "$jdbcprotocol" ".octo.out"
		local query_file
		for nocomment_query_file in "$fname"_nocomment-*.sql
		do
			# Note that "run_multiple_query_files.java" produces output even for DML/DDL commands (e.g. INSERT INTO)
			# and not just for "SELECT" so it is safe to assume that the *.psql.out and *.octo.out files exist.
			# Postgres displays boolean values as t/f whereas Octo displays them as 1/0 so fix that before the diff.
			cat $nocomment_query_file.psql.out | sed 's/\<t\>/1/g;s/\<f\>/0/g;' >& $nocomment_query_file.ref
			query_num=`echo $nocomment_query_file | sed 's/.*_nocomment-//g'`
			query_file="$fname"_comment-"$query_num"
			# Distinction between commented and non-commented query file is required
			# because output processing utilizes commented instructions for Octo Postgres comparison
			run_query_verify $query_file $nocomment_query_file
		done
	else
		query_file="$1"
		local fname=`echo $query_file | sed 's/\..*//g'`
		# Additionally, DROP TABLE IF EXISTS queries issue a NOTICE that the table does not exist. We do not want this
		# as it makes comparing Postgres vs Octo output harder so disable that using the PGOPTIONS variable below.
		PGOPTIONS='--client-min-messages=warning' $psql_cmd --no-align $database -f $query_file >& $fname.psql.out
		# Postgres displays boolean values as t and f whereas Octo displays them as 1 and 0 so fix that before the diff.
		if [[ $randclientstr == "PSQL" ]]; then
			run_psql $test_port $query_file --no-align > $fname.octo.out.tmp 2> $fname.octo.out.err
		else
			# OCTO
			octo -f $query_file >& $fname.octo.out.tmp
		fi
		# If it is a SELECT or VALUES query, then trim header information from Postgres and Octo output for effective
		# comparison. This is because the column names in the select column list might not match between Octo and Postgres
		# most of the times (sometimes they might match, but be in different case etc.). Note that SET operations might
		# start with "(SELECT ...) UNION (...)" therefore you need to also check for "(select" below.
		# For all other queries like DROP TABLE, CREATE TABLE, INSERT INTO, DELETE FROM, no need to do this trimming.
		querytype=`awk 'NR==1 {print tolower($1);}' $query_file`
		if [[ "select" == $querytype || "(select" == $querytype || "values" == $querytype ]]; then
			tail -n +2 $fname.octo.out.tmp | sed 's/\<t\>/1/g;s/\<f\>/0/g;' &> $fname.octo.out
			tail -n +2 $fname.psql.out | sed 's/\<t\>/1/g;s/\<f\>/0/g;' &> $fname.ref
		else
			sed 's/\<t\>/1/g;s/\<f\>/0/g;' $fname.octo.out.tmp &> $fname.octo.out
			sed 's/\<t\>/1/g;s/\<f\>/0/g;' $fname.psql.out >& $fname.ref
		fi
		run_query_verify $query_file $fname
	fi
	return
}

sort_LC_ALL_C() {
	# We have noticed that setting LC_ALL to en_US.UTF8 (done by the test framework at start of each subtest in "init_test()")
	# results in different sort output (in rare cases in TQG04 subtest, and in all cases in the TRTE07 subtest). This is
	# suspected to be a bug in older versions of sort (sort 8.22 and older which is the version of sort in Ubuntu 18.04
	# and RHEL 7). sort 8.30 (version of sort in Ubuntu 20.04, CentOS 8 etc.) works fine in this case.
	# Therefore, we work around this issue by setting the locale to C just for the duration of the sort.
	# just for the "sort" command.
	export LC_ALL=C
	sort
	export LC_ALL=en_US.UTF8
}

run_query_verify() {
	local query_file="$1"
	local fname="$2"
	sed -i '/^.*\[ INFO\].* \/.*\/octo.conf/d' $fname.octo.out
	cat $fname.octo.out >& $fname.log
	if [[ $trimzeroes == "trim_trailing_zeroes" ]]; then
		# Remove floating point numbers with extraneous 0s at the end
		# Postgres creates these floating point numbers if an AVG aggregate function is invoked whereas Octo does not.
		mv $fname.ref $fname.ref1
		mv $fname.log $fname.log1
		# The below step converts for example : 2.5000000000000000 -> 2.5
		pat1='s/\(\.[0-9]*[1-9]\)0\{1,\}$/\1/;s/\(\.[0-9]*[1-9]\)0\{1,\}|/\1|/g;'
		sed $pat1 $fname.ref1 > $fname.ref2
		sed $pat1 $fname.log1 > $fname.log2
		# The below step converts for example : 1.0000000000000000 -> 1
		pat2='s/\.0\{1,\}$//;s/\.0\{1,\}|/|/g;'
		sed $pat2 $fname.ref2 > $fname.ref3
		sed $pat2 $fname.log2 > $fname.log3
	else
		# If $trimzeroes != "trim_trailing_zeroes", still move the files into their "ref3" and "log3"
		# versions for use later on in the process.
		mv $fname.ref $fname.ref3
		mv $fname.log $fname.log3
	fi
	# Below two sed commands add leading zeros into the Octo output in order to match PSQL's output
	# Octo Output: .333   PSQL Output: 0.333
	sed 's/|\.\([0-9]*\)/|0.\1/;' $fname.log3 > $fname.log4
	sed 's/^\.\([0-9]*\)/0.\1/;' $fname.log4 > $fname.log5
	# Check if query has ORDER BY that is not inside a subquery OR if "--sort-needed-check" is present. If so, do not sort.
	# Otherwise sort as order is not guaranteed to match between Octo and Postgres.
	# grep -c can return non-zero status in case of no match hence the need for "|| true"
	orderbyexists=$(yottadb -r removeInnerQueries $query_file | grep -ci "ORDER BY") || true
	sortneededcheck=$(grep -c "sort-needed-check" $query_file) || true
	if [[ $orderbyexists -eq 0 ]] || [[ $sortneededcheck -ne 0 ]]; then
		# ORDER BY does not exist in the query. So sort the outputs.
		mv $fname.ref3 $fname.unsorted.ref3
		mv $fname.log5 $fname.unsorted.log5
		cat $fname.unsorted.ref3 | sort_LC_ALL_C > $fname.ref3
		cat $fname.unsorted.log5 | sort_LC_ALL_C > $fname.log5
	fi
	# Below two seds set numerical values to only have 10 digits after the decimal point as both Octo
	# and PSQL can return varying levels of precision (12 is the least amount of decimals seen so far)
	sed -re 's/([0-9]+\.[0-9]{10})[0-9]+/\1/g' $fname.ref3 >& $fname.ref
	sed -re 's/([0-9]+\.[0-9]{10})[0-9]+/\1/g' $fname.log5 >& $fname.log
	# grep -c can return non-zero status in case of no match hence the need for "|| true"
	rowcountonlycheck=$(grep -c "rowcount-only-check" $query_file) || true
	if [[ $rowcountonlycheck -eq 0 ]]; then
		# diff result is captured to stop_rocto before issuing a test fail
		diff $fname.ref $fname.log >& $fname.diff || true
		if [[ -s $fname.diff ]]; then
			run_query_stop_rocto
			exit 1
		fi
	else
		# .sql file requests only rowcount check. No exact content check.
		# Check only # of lines in output match. Not contents of lines.
		psqllines=$(wc -l $fname.ref | awk '{print $1}')
		octolines=$(wc -l $fname.log | awk '{print $1}')
		if [[ $psqllines -ne $octolines ]]; then
			echo "Expected $psqllines lines but found $octolines lines in $fname.log"
			run_query_stop_rocto
			exit 1
		fi
	fi
}

run_query_stop_rocto() {
	# This function is only used inside the test framework
	if [[ $randclientstr != "OCTO" ]]; then
		if [[ ! -z $isnotexternal ]];then
			stop_rocto
			# Reset rocto port related variables so the next "if [[ -z "$test_port" ]];"
			# check in "run_query_in_octo_and_postgres_and_crosscheck_multiple_queries"
			# function will correctly invoke "start_rocto" like it should.
			isnotexternal=
			test_port=
		fi
	fi
	randclientstr=
}

run_query_generator() {
  databaseName="$1"
  prefix="$2"
  usejdbcoption="$3"
  useextended="$4"
  numQueries=100	# For now generate 100 queries by default (good balance of test run time vs test coverage)

  if [[ "" != $usejdbcoption ]]; then
	numQueries=$(( $numQueries / 4 ))	# Generate 25% of queries to save time in the pipeline for JDBC tests
  fi

  cp @PROJECT_SOURCE_DIR@/tests/fixtures/QueryGenerator.m .
  load_fixture "$databaseName.sql"
  load_fixture "$databaseName.zwr"
  load_fixture "QueryGenerator.sql"
  load_postgres_fixture $databaseName postgres-QueryGenerator.sql

  $ydb_dist/yottadb -run QueryGenerator "$databaseName.sql" "$databaseName.zwr" "$numQueries" "$prefix"

  count=0
  queryfile=$prefix$count
  queryfile+=".sql"
  run_query_in_octo_and_postgres_and_crosscheck_multiple_queries $databaseName $queryfile "trim_trailing_zeroes" $usejdbcoption $useextended
}
