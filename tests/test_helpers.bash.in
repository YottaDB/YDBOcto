#################################################################
#								#
# Copyright (c) 2019-2023 YottaDB LLC and/or its subsidiaries.	#
# All rights reserved.						#
#								#
#	This source code contains the intellectual property	#
#	of its copyright holder(s), and is made available	#
#	under a license.  If you do not know the terms of	#
#	the license, please stop and do not read further.	#
#								#
#################################################################

setup_bats_env() {
	export test_temp=$(mktemp -d @TEST_OUTPUT_DIR@/bats-test.XXXXXX)
	echo "Temporary files in: $test_temp"
	exec >  >(tee -ia $test_temp/stdout.txt)
	exec 2> >(tee -ia $test_temp/stderr.txt >&2)
	cd $test_temp
	# For expect tests, xterm may try to add more control characters. Avoid that by setting a minimal terminal.
	# https://gitlab.com/YottaDB/DBMS/YDBOcto/-/issues/717
	export TERM=dumb
	# Disable any user-level (~/.inputrc) customizations that can cause test failures (e.g. TERR008, TSC18 subtests)
	# For example "set editing-mode vi" in ~/.inputrc somehow re-enables tab-completion even though Octo disables it
	#	by calling `rl_bind_key('\t', rl_insert);`
	export INPUTRC=/etc/inputrc
	# Below has been seen to avoid Ctrl-M ('^M') characters in long query strings
	export COLUMNS=4096
	# Create a a log of the test being run
	echo " --> Running test [$BATS_TEST_FILENAME] : subtest [$BATS_TEST_DESCRIPTION] : in $PWD : build type @CMAKE_BUILD_TYPE@ " > bats_test.out
	if [ ! -x "$(which rpm)" ] || ! grep -q 'VERSION_ID="7' /etc/os-release; then
		is_rhel7=false
	else
		is_rhel7=true
	fi
	skip_escape_sequence_sanitize=0	# By default do not skip RHEL 7 specific escape_sequence_sanitize() call in verify_output()
}

save_env_variables() {
	# Note: The below set of lines also exist in `tools/ci/build.sh` so any change here might need to be made there too.
	# Log env vars, shell vars, locale info in files for later analysis of test failures.
	# Mask any sensitive env vars out.
	env | grep -vE "HUB_USERNAME|HUB_PASSWORD|CI_JOB_TOKEN|CI_REGISTRY_PASSWORD|CI_BUILD_TOKEN|CI_REPOSITORY_URL" > dbg_env.out
	set | grep -vE "HUB_USERNAME|HUB_PASSWORD|CI_JOB_TOKEN|CI_REGISTRY_PASSWORD|CI_BUILD_TOKEN|CI_REPOSITORY_URL" > dbg_set.out
	locale > dbg_locale.out
	locale -a > dbg_localeall.out
	# This exports an environment that can be easily sourced to examine the test interactively
	declare -p -x | grep -- "-x ydb_" | grep -v ydb_unset_ | grep -v ydb_sav_ > test.env
}

set_ydb_xc_octo() {
	# Set the ydb_xc_octo environment variable to allow M callouts to C functions needed in some tests
	if [[ @DISABLE_INSTALL@ == "ON" ]]; then
		# ydbocto.xc specifies the callout library as $ydb_dist/plugin/libcocto.so by default.
		# However, if installation is disabled, this file won't exist at this location. So,
		# create a new .xc file that specifies the location of libcocto.so. Note that a new
		# file is created to avoid needing to revert ydbocto.xc to the default libcocto.so
		# installation path at tarball generation time in build.sh.
		cp @PROJECT_BINARY_DIR@/src/ydbocto.xc @PROJECT_BINARY_DIR@/src/ydbocto_noinstall.xc
		libcocto=@PROJECT_BINARY_DIR@/src/libcocto.so
		sed -i "12s,.*,$libcocto," @PROJECT_BINARY_DIR@/src/ydbocto_noinstall.xc
		export ydb_xc_octo="@PROJECT_BINARY_DIR@/src/ydbocto_noinstall.xc"
	else
		export ydb_xc_octo=$ydb_dist/plugin/octo/ydbocto.xc
	fi
	echo "ydb_xc_octo=$ydb_xc_octo" >> env.out
}

init_test() {
	setup_bats_env

	# Set YDB related env vars
	unset ydb_gbldir gtmgbldir	# needed or else ydb_env_set can issue ZGBLDIRACC error (due to it calling MUPIP DUMPFHEAD)
					# if ydb_gbldir is defined and points to a non-existent gld file.
	# The below is needed to ensure that the bats tests run fine even if run concurrently.
	# Without setting ydb_dir to the current directory (which is unique for each test/subtest),
	# concurrent calls to ydb_env_set would create/update/delete journal files under ~/.yottadb/r*/g/yottadb.mjl
	# at the same time which would cause mysterious errors (e.g. %YDB-E-JNLFILEOPNERR) in random tests depending on timing.
	export ydb_dir=.
	# Pipeline jobs start with `gtmdir` set (to "/data") so fix that to be in sync with `ydb_dir`
	# or else `ydb_env_set` would issue a %YDBENV-F-YDBGTMMISMATCH error.
	export gtmdir=$ydb_dir
	# Randomly choose whether to use UTF-8 or M mode
	source @YOTTADB_INCLUDE_DIRS@/ydb_env_unset
	rand_chset=$(( $RANDOM % 2))
	# We have seen various subtests fail occasionally with a LeakSanitizer SIG-11 when run in the "asan-ubuntu"
	# pipeline job or when linking Octo with YottaDB that was built using ASAN. All the failure runs so far have
	# happened when ydb_chset was randomly set to "UTF-8". For lack of a better fix/workaround, we set ydb_chset to M
	# for all subtests when run with ASAN enabled. See https://gitlab.com/YottaDB/DBMS/YDBOcto/-/issues/892#note_1156225457
	# and other threads in the same issue for details.
	if [[ @ENABLE_ASAN@ == "ON" ]]; then
		rand_chset=1
	fi
	if [[ $rand_chset -eq 0 ]]; then
		export ydb_chset=UTF-8
		utf8_path="/utf8"
	else
		export ydb_chset=M
		utf8_path=""
		# Set ydb_icu_version to the right value even if we chose M mode. This way, wherever we set ydb_chset to "UTF-8"
		# (e.g. in "load_fixture()" function while loading the northwind.zwr fixture) things will work fine.
		# Not setting this can cause ICUSYMNOTFOUND errors. No need to do this for the case when "ydb_chset" is chosen
		# to be "/utf8" as the "ydb_env_set" call done after this if/else block will set that env var.
		ydb_icu_version=$(readlink /usr/lib*/libicuio.so /usr/lib*/*/libicuio.so | sed 's/libicuio.so.\([a-z]*\)\([0-9\.]*\)/\2.\1/;s/\.$//;')
		export ydb_icu_version
	fi
	source @YOTTADB_INCLUDE_DIRS@/ydb_env_set
	# Run tests locally without installing Octo/Rocto to $ydb_dist
	if [[ @DISABLE_INSTALL@ == "ON" ]]; then
		export PATH="@PROJECT_BINARY_DIR@/src:$PATH"
		# tests/fixtures has lots of M programs. Make sure .o files for those get created in current test output directory
		# (and not in tests/fixtures) as it can otherwise cause .o file format issues (e.g. if YDB changes the M .o file format)
		# since tests/fixtures persists a lot longer than the test output directory. We had previously seen COLLATIONUNDEF errors
		# while trying to use the master branch of YDB repo when .o files created by r1.29 in Nov 2019 were tried to be used
		# by r1.29 in Dec 2019 (after the .o file format was changed in between). Having the .o files in the test output directory
		# avoids such issues. This comment also applies to the similar line in the below `else` block.
		ydbroutines=".(. @PROJECT_SOURCE_DIR@/tests/fixtures) @PROJECT_BINARY_DIR@/src$utf8_path/_ydbocto.so"
	else
		# Add the UTF-8 path, if set, as this is not done by build.sh in the pipeline
		ydbroutines=".(. @PROJECT_SOURCE_DIR@/tests/fixtures)"
	fi
	export ydb_routines="$ydbroutines $ydb_routines"

	# Set env var to allow .m and .o file timestamps to be identical. This is expected to be particularly useful
	# in the pipeline test runs where we have seen the _ydboctoP*.o file almost always created with a timestamp
	# that is 1 second later than the corresponding _ydboctoP*.m file. We suspect this is due to the underlying
	# file system granularity being at 1-second (instead of 1-milli/micro/nano second). Once this env var is
	# set, YDB will not wait for the .o file time stamp to be different than the .m file. Since there are almost
	# thousands of queries that run in the pipeline tests lots of these 1-second delays adds up to minutes of
	# slowdown which should all hopefully go away with this env var set.
	export ydb_recompile_newer_src=TRUE
	# Below is needed to avoid wildcard expansion order (e.g. cat *.m used in various tests) changing based on LC_TYPE
	# It is also needed to avoid sort order changing (and reference file issues) based on LC_CTYPE being "C" or "en_US.UTF8".
	# Fixing LC_ALL to a particular value ensures caller environment does not affect test results.
	export LC_ALL=en_US.UTF8
	# Avoid generating .pyc files during tests, which clutter up the source directory
	# All the python scripts take a very small fraction of the test time,
	# so this does not impact pipeline times in any significant way.
	export PYTHONDONTWRITEBYTECODE=1
	# Set env var to maintain list of common schemas to create across different database emulations, e.g. Postgres and MySQL
	export schema_list="names names1col nameslastname customers pastas easynames northwind $(echo sqllogic{1,2,3,4,5}) boolean nullnames nullcharnames composite quotenames"

	# ASAN Options if ASAN is enabled
	if [[ @ENABLE_ASAN@ == "ON" ]]; then
		export ASAN_OPTIONS="abort_on_error=1:disable_coredump=0:unmap_shadow_on_exit=1:verify_asan_link_order=0:suppressions=@PROJECT_SOURCE_DIR@/tests/fixtures/asan-supp.txt"
		export LSAN_OPTIONS="suppressions=@PROJECT_SOURCE_DIR@/tests/fixtures/lsan-supp.txt:print_suppressions=0"
	fi
}

init_tls() {
	# Generate CA key and certificate
	openssl genpkey -algorithm RSA -pkeyopt rsa_keygen_bits:2048 -pass pass:tester -out $test_temp/CA.key
	openssl req -new -nodes -key $test_temp/CA.key -passin pass:tester -days 365 -x509 \
			-subj "/C=US/ST=PA/L=Malvern/O=Octo/CN=www.yottadb.com" -out $test_temp/CA.crt
	# Create server key and certificate request
	openssl genpkey -algorithm RSA -pkeyopt rsa_keygen_bits:2048 -pass pass:tester -out $test_temp/server.key
	openssl req -new -key $test_temp/server.key -passin pass:tester \
		-subj "/C=US/ST=PA/L=Malvern/O=Octo/CN=www.yottadb.com" -out $test_temp/server.csr
	# Sign certificate based on request and local CA
	openssl x509 -req -in $test_temp/server.csr -CA $test_temp/CA.crt -CAkey $test_temp/CA.key -CAcreateserial \
		-out server.crt -days 365
	# Pass private key password to environment variable
	echo tester | $ydb_dist/plugin/gtmcrypt/maskpass | cut -f 3 -d " " >> env.log
	export ydb_tls_passwd_OCTOSERVER=$(echo tester | $ydb_dist/plugin/gtmcrypt/maskpass | cut -f 3 -d " ")
	export ydb_crypt_config=$test_temp/octo.conf
	cat <<OCTO &>> $test_temp/octo.conf
rocto: {
	ssl_on: true;
}

tls: {
	CAfile: "$test_temp/CA.crt";
	CApath: "$test_temp/";
	OCTOSERVER: {
		format: "PEM";
		cert: "$test_temp/server.crt";
		key: "$test_temp/server.key";
	}
}
OCTO
}

copy_test_files() {
	for f in $@; do
		mkdir -p $test_temp/$(dirname $f)
		cp @PROJECT_SOURCE_DIR@/tests/$f $test_temp/$f
	done
}

# load_fixture <fixture name, relative to tests/fixtures
load_fixture() {
	fixture_name=$1
	echo "Loading fixture $fixture_name"
	if [[ $fixture_name == *.zwr ]]; then
		# Determine the chset (M or UTF-8) to use from the extract file. It will be either "UTF-8" or "".
		chset=`grep "MUPIP EXTRACT" @PROJECT_SOURCE_DIR@/tests/fixtures/$fixture_name | awk '{print $4}'`
		# Keep LC_ALL as en_US.UTF8 (set in "init_test()" function) for both M and UTF-8 mode. That should work just fine.
		ydb_chset=$chset $ydb_dist/mupip load @PROJECT_SOURCE_DIR@/tests/fixtures/$fixture_name
	elif [[ $fixture_name == *.sql ]]; then
		if [[ $2 == "subtest" ]]; then
			cp @PROJECT_SOURCE_DIR@/tests/fixtures/$fixture_name $fixture_name
			grep -v '^#' $fixture_name 2>&1 | tee -a output.txt	   # Filter out copyright from output
			grep -vE '^-- |^#' $fixture_name > input.sql || [ $? = 1 ] # Filter out comment lines as they otherwise clutter the parse error output (if any)
			if [[ $3 == "novv" ]]; then
				octoflags="$4"
			elif [[ $3 =~ "v" ]]; then	# Allow verbosity level to be passed directly
				octoflags="-$3 $4"
			else
				octoflags="-vv $4"
			fi
			octo $octoflags -p -f input.sql 2>&1 | tee -a output.txt
		else
			octo -f @PROJECT_SOURCE_DIR@/tests/fixtures/$fixture_name
		fi
	else
		echo "Unrecognized file extension: $fixture_name"
		exit 1
	fi
}

load_rocto_fixture() {
	fixture_name=$1
	rocto_port=$2
	echo "Loading fixture $fixture_name"
	if [[ $fixture_name == *.sql ]]; then
		cp @PROJECT_SOURCE_DIR@/tests/fixtures/$fixture_name $fixture_name
		grep -v '^#' $fixture_name 2>&1 | tee -a output.txt	   # Filter out copyright from output
		grep -vE '^-- |^#' $fixture_name > input.sql || [ $? = 1 ] # Filter out comment lines as they otherwise clutter the parse error output (if any)
		run_psql $rocto_port < input.sql 2>&1 | tee -a output.txt
	else
		echo "Unrecognized file extension: $fixture_name"
		exit 1
	fi
}

create_postgres_database() {
	psql postgres <<PSQL &> $1.setup.out
	-- Need to use LC_COLLATE='C' to ensure string comparison of 'Z' < 'a' returns TRUE (default is en_US.UTF8)
	-- Need template=template0 to avoid the following error
	-- "new collation (C) is incompatible with the collation of the template database (en_US.UTF-8)"
	create database $1 LC_COLLATE='C' template=template0;
PSQL
	# Create a PostgreSQL role for the user running the tests and grant that user sufficient PostgreSQL permissions to
	# log in to the specified database and access all the tables there. Note that ALTER is used for the case when the user
	# already exists in PostgreSQL, but doesn't have the required settings.
	psql $1 <<PSQL 2>&1 | tee -a $1.setup.out
	CREATE ROLE $USER;
	ALTER ROLE $USER LOGIN;
	ALTER ROLE $USER PASSWORD 'ydbrocks';
	GRANT CONNECT ON DATABASE $1 TO $USER;
	GRANT ALL PRIVILEGES ON ALL TABLES IN SCHEMA public TO $USER;
PSQL
}

# This function sets $? to - if a postgres database named $1 exists. If not, it sets $? to 1.
postgres_database_exists() {
	# Check if $1 is a valid Postgres database name first. Do this separately as the next step has a "|| true"
	# to account for ON_ERROR_STOP and we do not want errors from a non-existent table name to be swallowed there.
	# See https://stackoverflow.com/a/16783253 for more details on the below.
	# If a database with name $1 does not exist, the below step would set $? to 1.
	psql -lqt | cut -d \| -f 1 | grep -qw $1
}

load_postgres_fixture() {
	postgres_database_exists $1 # If a Postgres database with the name $1 does not exist, exit right here with an error.
	# Now that we know $1 is a valid table, run the file $2 that creates tables and loads data for the database $1.
	# If the table already exists, we want to do nothing here.
	# The way we achieve this is by setting "ON_ERROR_STOP" to "on" before doing the "CREATE TABLE" inside the $2 file.
	# This way the "CREATE TABLE" will fail if the table already exists and we will skip the INSERT commands.
	# An alternative way to avoid the "ON_ERROR_STOP" is to "DROP TABLE IF EXISTS" first and then run the "CREATE TABLE".
	# But this has the issue that any table privileges that were added for multiple users will get removed and have to be
	# added again. Additionally, if tests are run concurrently, it is possible a test using this Postgres table run
	# at the same time the table is being dropped/created leading to mysterious test failures due to a missing/incomplete
	# table. Hence we stick with the "ON_ERROR_STOP" approach. Since this can return a non-zero exit status and that is
	# expected, we use the "|| true" below to avoid the bats framework from treating this as a failure and stopping the test.
	psql $1 -c '\set ON_ERROR_STOP on' -f @PROJECT_SOURCE_DIR@/tests/fixtures/$2 &> $1.psql.load || true
}

run_mysql() {
	# When running in the pipeline the host must be specified as "mysql", since
	# localhost is inaccessible from within the GitLab CI Docker container. When
	# run outside of the pipeline, localhost is suitable.
	if [[ $USER == "root" ]]; then
		host=mysql
	else
		host=localhost
	fi
	# Specify password by environment variable to avoid MySQL warning on command-line password security, which breaks outrefs
	MYSQL_PWD="ydbrocks" mysql --host=$host --protocol=TCP --user=$USER $1 < $2
}

create_mysql_database() {
	echo "-- Need to use LC_COLLATE='C' to ensure string comparison of 'Z' < 'a' returns TRUE (default is en_US.UTF8)" &> in.sql
	echo "CREATE DATABASE IF NOT EXISTS $1;" &> in.sql
	run_mysql "" in.sql &> $1.setup.out
	# Set the timezone explicitly for consistency when running in pipeline, as there is no guarantee that
	# MySQL will actually use the system timezone and the actual timezone cannot be manually queried, either.
	# For more information, see:
	# https://stackoverflow.com/questions/2934258/how-do-i-get-the-current-time-zone-of-mysql/2934271#2934271
	echo "SET GLOBAL time_zone = '$(date +%:z)';" >> tz.sql
	run_mysql "" tz.sql >> $1.setup.out
}

load_mysql_fixture() {
	# Create a MySQL table and load data into it
	run_mysql $1 @PROJECT_SOURCE_DIR@/tests/fixtures/$2 &> $3
}

createdb() {
	# Need to have non-default key size specified as otherwise it won't fit
	# VistA globals.  Technically, VistA only needs the M standard's
	# maximum of 256 whereas the default maximum in GDE is 64. But the
	# maximum possible key size of 1019 does not hurt.
	export ydb_gbldir="yottadb.gld"
	echo "ydb_gbldir: $ydb_gbldir"
	# Create two regions by default. One to hold application/user data. One to hold Octo internal data (%ydbocto* namespace)
	# But if user specified number of regions explicitly (only allowed value currently is 1) then create 1 region only.
	if [[ $1 == "1" ]]; then
		$ydb_dist/yottadb -run ^GDE <<FILE
		change -r DEFAULT -key_size=1019 -record_size=1048576
		change -segment DEFAULT -file_name=$test_temp/mumps.dat
		change -r DEFAULT -NULL_SUBSCRIPTS=true
		exit
FILE
	else
		$ydb_dist/yottadb -run ^GDE <<FILE
change -region DEFAULT -null_subscripts=true -record_size=1048576 -key_size=1019
change -segment DEFAULT -file_name="$test_temp/mumps.dat"
add -segment OCTOSEG -file="$test_temp/octo.dat"
add -region OCTOREG -dyn=OCTOSEG -null_subscripts=true -key_size=1019 -record_size=1048576
add -segment AIMSEG -file="$test_temp/aim.dat" -access_method=MM -block_size=2048
add -region AIMREG -dyn=AIMSEG -nojournal -key_size=1019 -null_subscripts=always -record_size=2048
add -name %ydbocto* -region=OCTOREG
add -name %ydbAIM* -region=AIMREG
exit
FILE
	fi
	rm *.dat || true
	$ydb_dist/mupip create
	# Set the below env var to ensure no DBFILEXT messages show up in syslog and in turn in individual test output files
	# e.g. if a test redirects stderr to a file output.txt, then syslog messages would show up in that file if run through
	# the pipeline causing a test failure if that is compared against a reference file (e.g. TC001 in test_create_table.bats.in)
	export ydb_dbfilext_syslog_disable=1

	save_env_variables

}

gde_add_region() {
	$ydb_dist/yottadb -run GDE <<FILE
add -name $2 -region=$1
add -region $1 -dynamic=$1 -key_size=1019 -record_size=1048576
add -segment $1 -file_name=$test_temp/$1.dat
exit
FILE
	$ydb_dist/mupip create -region=$1
}

gde_add_name() {
	echo "NAME: $1"
	$ydb_dist/yottadb -run GDE <<FILE
add -name $1 -region=$2
exit
FILE
}

set_null_subs() {
	$ydb_dist/mupip set -region $1 -NULL_SUBSCRIPTS=$2
}

find_open_port() {
	local start_port=$1
	# We need to find an open port on the system.
	# Note that it is possible tests are run concurrently. In that case,
	# this function can be invoked at the same time from different processes.
	# Therefore we need to get a lock and execute the below code.

	local lockfile=@TEST_OUTPUT_DIR@/find_open_port
	# Find a free file descriptor in the current shell process first (using {var} syntax in bash)
	exec {lockfd}>$lockfile.lock
	# At this point "$lockfd" contains the value of the fd in the current shell process
	# which was obtained as a result of opening the file "$lockfile.lock"

	# Now lock this file descriptor to effectively lock the file.
	flock $lockfd

	# ---------------- BEGIN : Code that runs under the exclusive lock -----------------

	# Need to switch to portno allocation .gld/.dat files for below commands (to find an open port) hence the "ydb_gbldir="
	# env var set just before each command invocation below. This avoids having to store the env var "ydb_gbldir" value (set
	# by createdb()) in a local variable, switch the env var to the portno allocation .gld and then switch it back to the
	# saved value at the end of these commands.

	# If not already done, create database file that stores portno allocations
	if ! [ -e $lockfile.gld ]; then
		ydb_gbldir=$lockfile.gld $ydb_dist/yottadb -run GDE "change -segment DEFAULT -file_name=$lockfile.dat"
		ydb_gbldir=$lockfile.gld $ydb_dist/mupip create
	fi

	# Find open port
	open_port=$(ydb_gbldir=$lockfile.gld $ydb_dist/yottadb -run findopenport^portno $start_port)

	echo $open_port
	# Record free port number that was found and time when it was found for later debugging in case it is needed
	echo " `date -Ins` : Allocated port [$open_port]" >> portno.out

	# ---------------- END  : Code that runs under the exclusive lock -----------------

	# Drop lock on the file
	flock -u $lockfd
}

run_octo_allow_error() {
	if [[ ! -f $1 ]]; then
		# If the input file isn't present in the current directory, it's a fixture.
		# So, strip the comments and copy it into the working directory for use below.
		strip_sql_comments $1
	fi
	# Run octo, but don't fail the test when an error is encountered. Used for testing error cases.
	octo $3 -f $1 >> $2 2>&1 || true
}

start_rocto() {
	create_user $USER "ydbrocks" --readwrite --allowschemachanges
	echo "ROCTO START" >> env.log
	if [[ $1 != "" ]]; then
		rocto_port=$(find_open_port $1)
		portspec="-p $rocto_port"
		shift 1
	else
		rocto_port=""
		portspec=""
	fi
	if [[ $1 == "quiet" ]]; then
		verbosity="-v"
		shift 1
	elif [[ $1 == "verbose" ]]; then
		verbosity="-vvv"
		shift 1
	else
		verbosity="-vv"
	fi
	if [[ -e rocto.log ]]; then
		mv rocto.log rocto.log_`date -Ins`_$$
	fi
	if [[ -e rocto.stdout ]]; then
		mv rocto.stdout rocto.stdout_`date -Ins`_$$
	fi
	# We are about to start a "rocto" listener process in the background below . If the current subtest fails due to an
	# octo/rocto issue (e.g. a SIG-11 in the rocto server process that causes the run_psql or run_java test helper scripts
	# to exit with a non-zero status), bats would try to exit the subtest right then but will hang because the rocto listener
	# process still has an open file descriptor that bats cares about. The "3>&-" is needed below to close that. See
	# https://bats-core.readthedocs.io/en/stable/writing-tests.html#file-descriptor-3-read-this-if-bats-hangs for more details.
	rocto $verbosity $portspec $* 1> rocto.stdout 2> rocto.log 3>&- &
	rocto_pid=$!
	echo $rocto_pid > rocto.pid
	# Store rocto port number in the file "rocto.port" for later use in "stop_rocto"
	echo $rocto_port > rocto.port
	while [[ ! -e rocto.log || "$(grep -c "rocto started" rocto.log)" == "0" ]]; do
		sleep .1s
		# If the rocto job disappears while checking, signal failure.
		if [[ ! -e /proc/$rocto_pid ]]; then
			return -1
		fi
	done
	echo $rocto_port
}

wait_for_rocto_server_pids_to_die() {
	listener_pid=$(cat rocto.pid)
	# Wait for rocto servers (i.e. children of the listener) to die
	# The grep is to filter out non-rocto processes that can start with ASAN
	# With clang, we found that llvm-symbolizer starts as a child process of rocto and won't die on its own
	ps --no-headers --ppid $listener_pid | grep rocto | awk '{print $1}' | while read server_pid; do
		# Wait for rocto server process to actually die
		tail --pid=$server_pid -f /dev/null
	done
}

stop_rocto() {
	# Wait for rocto listener to die
	if [[ -e rocto.pid ]]; then
		listener_pid=$(cat rocto.pid)
		# Signal listener rocto process to die
		$ydb_dist/mupip stop $listener_pid
		wait_for_rocto_server_pids_to_die
		# Wait for listener rocto process to actually die
		tail --pid=$listener_pid -f /dev/null
		# Release port number held by rocto (stored in the file "rocto.port")
		local lockfile=@TEST_OUTPUT_DIR@/find_open_port
		local test_port=`cat rocto.port`
		ydb_gbldir=$lockfile.gld $ydb_dist/yottadb -run releaseport^portno $test_port
		# Record port number release time for later debugging in case it is needed
		echo " `date -Ins` : Released port [$test_port]" >> portno.out
	fi
}

run_psql() {
	if [[ $2 =~ ".sql" ]]; then
		# Load a fixture
		strip_sql_comments $2
		PGPASSWORD=ydbrocks psql $3 -U ydb "host=localhost port=$1" < $2
	else
		PGPASSWORD=ydbrocks psql $3 -U ydb "host=localhost port=$1"
	fi
}

run_psql_auth() {
	if [[ @YDB_TLS_AVAILABLE@ -eq 1 ]]; then
		PGPASSWORD=$2 psql --no-align -U $1 "host=localhost port=$3"
	else
		PGPASSWORD=$2 psql --no-align -U $1 "sslmode=disable host=localhost port=$3"
	fi
}

# Run psql using a specific username ($4) and password ($5)
run_psql_user() {
	if [[ $2 =~ ".sql" ]]; then
		# Load a fixture
		strip_sql_comments $2
		PGPASSWORD=$5 psql $3 -U $4 "host=localhost port=$1" < $2
	else
		PGPASSWORD=$5 psql $3 -U $4 "host=localhost port=$1"
	fi
}

run_psql_expect() {
	unset PGPASSWORD
	expect -d -f @PROJECT_SOURCE_DIR@/tests/fixtures/$1.exp $2 > expect.out 2> expect.dbg
}

create_user() {
	# Create the specified user ($1) if it doesn't exist already.
	if [[ 0 -eq $(yottadb -r %ydboctoAdmin show users | grep $1 | wc -l) ]]; then
		echo -en "$2\n$2" | yottadb -r %ydboctoAdmin add user $1 $3 $4 >> user.log
	fi
}

create_default_user() {
	# Create the default `ydb` user with permissions to create, drop, and modify tables
	create_user ydb ydbrocks --readwrite --allowschemachanges
}

delete_users() {
	for user in $@; do
		yottadb -r %ydboctoAdmin delete user $user
	done
}

setup_go() {
	mkdir -p "$test_temp/go/src/"
	export GOPROXY=https://proxy.golang.org/cached-only
	export GOPATH="$test_temp/go"
}

run_go() {
	cp -r @PROJECT_SOURCE_DIR@/tests/go/src/$1 $GOPATH/src/$1
	# As of Go 1.16, the default behavior is GO111MODULE=on but we want to keep using the old GOPATH way,
	# therefore set the below env var to Go not to use the Go Modules feature. This is not needed by
	# Go versions older the Go 1.16 but it does not hurt.
	export GO111MODULE=off
	go get $1
	go build $1
	$GOPATH/bin/$1 $2
	# Executables created by go occupy a noticeable part of the artifacts.zip file created in the pipeline.
	# They are not considered necessary at this point for test failure analysis so remove them.
	rm -f $1
}

run_java() {
	strip_sql_comments $3
	if [[ $1 == "run_query" ]]; then
		# Only append ".sql" for queries derived from fixtures, not for generated queries
		if [[ -f $3 ]]; then
			query=$(cat $3)
		else
			query=$(cat $3.sql)
		fi
	elif [[ $1 == "run_multi_query" ]]; then
		query=$3.sql
	elif [[ $1 == "run_multiple_query_files" ]]; then
		query=$3
	else
		query=""
	fi
	psql_jar="@PROJECT_BINARY_DIR@/postgresql.jar"
	if ! [ -e "$psql_jar" ]; then
		echo "error: $psql_jar does not exist. Cannot continue."
	fi
	java -classpath $psql_jar:@PROJECT_BINARY_DIR@/src/jocto.jar $1 $2 "$query" $4 $5 $6
}

strip_psql_header() {
	sed -i '/Password for user /,/^[[:space:]]*$/d' $1
}

strip_expect_artifacts() {
	sed -i '/^.*spawn \/bin\/bash.*$/d' $1
	sed -i '/^.*stty cols 4096.*$/d' $1
	sed -i '/^.*psql -U ydb -h localhost -p .*$/d' $1
	# Shell prompt would have the bats-test.* directory name in some platforms (e.g. CentOS).
	# Shell prompt would have the directory name as $PWD in some platforms (e.g. Ubuntu)
	# Account for that in the 2 lines below. The [#$] is to account for either a # or a $ as the shell prompt
	sed -i '/^.*bats-test\..*[#$].*$/d' $1
	sed -i '/^.*\$PWD[#$]*.*$/d' $1
}

# Filter out '#' comment lines and blank lines, if any
strip_sql_comments() {
	# Remove .sql extension (occurs when root caller is run_query_in_octo_and_postgres_and_crosscheck)
	filename=$(echo $1 | cut -d'.' -f 1)
	# Comments are only present for fixtures
	if [[ -f @PROJECT_SOURCE_DIR@/tests/fixtures/$filename.sql ]]; then
		cp @PROJECT_SOURCE_DIR@/tests/fixtures/$filename.sql $filename.in
		grep -v '^#' $filename.in | grep -v '^[[:space:]]*$' > $filename.sql
	fi
}

verify_plans_xrefs_gvns_triggers() {
	queryoutputfile=$1
	referencefile=$2
	mupipextractfile=$3
	# Note that since this framework function is called from various tests (with different scenarios),
	# it is possible that some steps below have NO output. In that case we do not want the bats to fail.
	# Hence the "|| true" usage below.
	echo '# Verify generated _ydbocto*.m and _ydbocto*.o plan files' >> $referencefile
	ls -1 _ydbocto*.{m,o} >> $referencefile || true
	echo "# Verify how many queries created new plans" >> $referencefile
	grep "execute SQL query" $queryoutputfile >> $referencefile || true
	$ydb_dist/mupip extract $mupipextractfile
	echo "# Verify db nodes correctly represent tables and plans (including xref plans) that rely on them" >> $referencefile
	grep -w "tableplans" $mupipextractfile | grep -vw output_columns >> $referencefile || true
	echo "# Verify db nodes correctly represent functions and plans that rely on them" >> $referencefile
	grep -w "plan_metadata" $mupipextractfile | grep -w "functions" >> $referencefile || true
	echo "# Verify db nodes for plans that are reusable (i.e. do not need to be regenerated)" >> $referencefile
	grep -w "plan_metadata.*output_key" $mupipextractfile >> $referencefile || true
	echo "# Verify db nodes correctly represent cross-references" >> $referencefile
	grep "^\^%ydbAIM" $mupipextractfile >> $referencefile || true
	echo "# Verify db nodes correctly represent src/obj dirs of generated plans (including xref plans)" >> $referencefile
	grep "plandirs" $mupipextractfile >> $referencefile || true
	echo "# Verify currently installed triggers" >> $referencefile
	# MUPIP TRIGGER -SELECT currently has an additional output line containing "Output File:" before the actual
	# list of triggers (if any are installed). This can be confusing in reference files so remove this.
	mupip trigger -select < /dev/null | grep -v "Output File:" >> $referencefile || true
	echo "" >> $referencefile
}

# Filter file path (either relative or absolute)
# Assumes `clean_output.txt` exists and modifies it in place.
# Modifies the given file in place.
filter_file_path() {
	sed -i 's# [/\.][^ ]*/.*\.[yc]:[0-9]*# PATH:LINENUM#' clean_output.txt
}

verify_no_octo_or_ydb_error() {
	[[ $(grep "\[ERROR\] " stderr.txt | wc -l) -eq 0 ]]
	[[ $(grep "\%YDB\-E\-" stderr.txt | wc -l) -eq 0 ]]
	[[ $(grep "\%YDB\-W\-" stderr.txt | wc -l) -eq 0 ]]
	[[ $(grep "\%YDB\-F\-" stderr.txt | wc -l) -eq 0 ]]
}

verify_output() {
	# We had cases where loading octo-seed.sql in "createdb()" issued a "ERR_CANNOT_CREATE_FUNCTION" error but
	# the test passed inspite of that because that output went to stderr.txt whereas the test only looked at output.txt.
	# Therefore, since this is a central function that is usually invoked at the end of a subtest, take this opportunity
	# to verify stderr.txt do not have any Octo or YottaDB errors in them (prefix of [ERROR]). Fail the test if they do.
	verify_no_octo_or_ydb_error

	echo "Comparing outref/$1.ref $2"
	copy_test_files outref/$1.ref
	cp $2 clean_output.txt

	# Universal filters
	# Filter time and dates
	sed -i 's/[0-9]\{4\}-[0-9]\{2\}-[0-9]\{2\}/DATE/g' clean_output.txt
	sed -i 's/[0-9]\{2\}:[0-9]\{2\}:[0-9]\{2\}/TIME/g' clean_output.txt
	filter_file_path
	# Filter rule #s and line #s printed by flex in TRACE verbosity level output
	sed -i 's/Entering state [0-9]*/Entering state sss/' clean_output.txt
	# The below line output format changed between bison 3.5.1 (Ubuntu 20.04) and bison 3.7 (Ubuntu 20.10)
	# Therefore filter it out of the reference file to avoid complications there.
	sed -i '/Stack now [0-9 ]*/d' clean_output.txt
	# Filter out "Reading a token" as it shows up differently in different bison versions
	sed -i 's/Reading a token: //g' clean_output.txt	# bison 3.5.1 format
	sed -i '/^Reading a token$/d' clean_output.txt		# bison 3.7.1 format
	sed -i 's/\(Reducing stack by rule\) \([0-9]*\) \((line\) [0-9]*)/\1 rrr \3 lll)/' clean_output.txt
	# Filter octo.conf from $ydb_dist and $HOME, i.e. not in the local directory
	# Cannot specify $ydb_dist directly as it varies depending on UTF-8 mode selection,
	# so just use "plugin/octo" as that is guaranteed to be in the $ydb_dist path.
	# Note also that the $ydb_dist case is unconditionally removed - otherwise this
	# message will cause outref discrepancies for the two DISABLE_INSTALL cases ("ON" and "OFF").
	sed -i "/\/.*\/plugin\/octo\/octo.conf/d" clean_output.txt
	sed -i "s,$HOME\/octo.conf,\$HOME\/octo.conf," clean_output.txt
	# Filter M routine name
	sed -i 's/_ydboctoP[A-Za-z0-9]*\.m/_ydboctoP*.m/g' clean_output.txt
	sed -i 's/_ydboctoX[A-Za-z0-9]*\.m/_ydboctoX*.m/g' clean_output.txt
	# Filter M routine name with %
	sed -i 's/\%ydboctoP[a-zA-Z0-9]*,/%ydboctoP*,/' clean_output.txt
	# Filter OBJ file name
	sed -i 's/_ydboctoP[A-Za-z0-9]*\.o/_ydboctoP*.o/g' clean_output.txt
	sed -i 's/_ydboctoX[A-Za-z0-9]*\.o/_ydboctoX*.o/g' clean_output.txt
	# Filter cursor number
	sed -i 's/cursor [0-9]*/CURSOR_NUM/' clean_output.txt
	# Filter ydb_* environment variables
	sed -i '/^.*\[ INFO\] PATH:LINENUM DATE TIME: INFO_ENV_VAR: # .*$/d' clean_output.txt
	# Convert non-deterministic $PWD into a deterministic one
	# Convert symbolic links into absolute paths
	cd -P $PWD
	sed -i 's,'$PWD',$PWD,g' clean_output.txt
	# Change ^%ydbAIMDuAwRJbyqDiW78aZmOOKGGE(0)="1617743959908264 247102 YottaDB r989 Linux x86_64" to
	#        ^%ydbAIMDuAwRJbyqDiW78aZmOOKGGE(0)="*YottaDB*"
	# This sed is too complicated, I won't try to explain it.
	sed -i 's/\(\^%ydbAIMD.*(0)=\).*/\1"*YottaDB*"/g' clean_output.txt
	# Convert PROJECT_SOURCE_DIR references to generic one
	sed -i 's,'@PROJECT_SOURCE_DIR@',SRCDIR,g' clean_output.txt
	# Convert YDB C source file/line references to generic one
	sed -i 's,\(called from module\) .*/\(sr_.*\) at line [0-9]*,\1 \2 at line LINE,' clean_output.txt
	# Filter version number
	sed -i 's/Octo version [0-9]\.[0-9]\.[0-9]/Octo version x\.x\.x/' clean_output.txt
	sed -i 's/Rocto version [0-9]\.[0-9]\.[0-9]/Rocto version x\.x\.x/' clean_output.txt
	# Filter git commit hash
	sed -i 's/Git commit: [0-9,a-f]*/Git commit: xxxx/' clean_output.txt
	# Filter git uncommitted changes
	sed -i 's/Uncommitted changes: .*/Uncommitted changes: false/' clean_output.txt
	# Filter forked process pid
	sed -i 's/rocto server process forked with pid [0-9]*/rocto server process forked with pid PID/' clean_output.txt
	# Convert psql prompt to root for compatibility when running tests on systems with different PostgreSQL user permissions
	# Specifically, some test machines do not run tests with full PostgreSQL user permissions and so receive a prompt with '>' instead of '#'
	sed -i 's/=>/=#/' clean_output.txt
	# Remove message "Running thread PID was not suspended. False leaks are possible."
	# https://github.com/google/sanitizers/issues/1479
	if [[ @ENABLE_ASAN@ == "ON" ]]; then
		sed -i '/was not suspended. False leaks are possible./d' clean_output.txt
	fi

	# Selectively process output
	for i in "$@"; do
		# Filter verbosity statements from octo and psql
		# a default octo install will have WARNING verbosity so INFO and DEBUG will need to be filtered for most tests
		if [[ $i == "noinfo" ]]; then
			sed -i '/\[ INFO\]\|^INFO:/d' clean_output.txt
		fi
		if [[ $i == "nodebug" ]]; then
			sed -i '/\[DEBUG\]\|^DEBUG:/,/^[[:space:]]*$/d' clean_output.txt
		fi
		if [[ $i == "nosenderror" ]]; then
			sed -i '/failed to send message/d' clean_output.txt
		fi
		if [[ $i == "noconnclose" ]]; then
			# Remove "connection closed cleanly" messages
			sed -i '/connection closed cleanly/d' clean_output.txt
		fi
		if [[ $i == "noprompt" ]]; then
			# Remove octo prompt, but keep remainder of line
			sed -i 's/^.*OCTO>//' clean_output.txt
		fi
		if [[ $i == "nopromptline" ]]; then
			# Remove octo prompt lines
			sed -i '/OCTO>/,$!d' clean_output.txt
		fi
		if [[ $i == "psql" ]]; then
			# filter socket info
			sed -i 's/\[[0-9]*\.[0-9]*\.[0-9]*\.[0-9]*:[0-9]*\]/\[SOCKET\]/' clean_output.txt
		fi
		if [[ $i == "noport" ]]; then
			# filter port from INFO messages in rocto
			sed -i 's/rocto started on port [0-9]*/\[PORT\]/' clean_output.txt
		fi
		if [[ $i == "noexpect" ]]; then
			# Remove expect script artifacts
			strip_expect_artifacts clean_output.txt
			strip_psql_header clean_output.txt
		fi
		if [[ $i == "stripreturns" ]]; then
			sed -i 's/\r//g' clean_output.txt
		fi
		if [[ $i == "striphex" ]]; then
			# Remove any hex values consisting of 2 to 8 hex characters (16-64 bits)
			sed -i 's/0x[0-9a-fA-F]\{2,16\}/HEX/' clean_output.txt
		fi
		if [[ $i == "noforcedhalt" ]]; then
			sed -i '/%YDB-F-FORCEDHALT/d' clean_output.txt
		fi
		if [[ $i == "noconfig" ]]; then
			# Remove Octo's "Loading config from ..." message while leaving other INFO messages intact
			sed -i '/Loading config from/d' clean_output.txt
		fi
		# The sort argument should be passed last to allow sorting of final output file
		if [[ $i == "sort" ]]; then
			mv clean_output.txt clean_output.txt.nosort
			cat clean_output.txt.nosort | sort_LC_ALL_C >& clean_output.txt
		fi
	done
	escape_sequence_sanitize
	# test_where_in.bats TWI12 when executed with JDBC driver fails without the following code.
	# This is because the semicolon at the end of a query is left out of the physical plan in this case.
	# To avoid such failure we remove all occurrences of semicolons before the comparison.
	# The first condition is required as not all tests create `dbg_env.out`(hello_db.bats.in is one
	# such test).
	if ([[ -e dbg_env.out ]]) && (grep -q "randclient:JDBC" dbg_env.out); then
		sed 's/;//g' outref/$1.ref > jdbc_$1.ref
		sed 's/;//g' clean_output.txt > jdbc_clean_output.txt
		diff jdbc_$1.ref jdbc_clean_output.txt
	else
		diff outref/$1.ref clean_output.txt
	fi
	return
}

escape_sequence_sanitize() {
	# On RHEL7 and CentOS7, the OCTO> prompt has some escape sequences before it (due likely to an older version of readline()).
	# Need that trimmed out as it will cause reference file issues.
	# But if the caller subtest has explicitly disabled this trimming (by setting a variable) honor that and skip this trimming.
	if [ $is_rhel7 ] && [ 1 != $skip_escape_sequence_sanitize ] ; then
		mv clean_output.txt clean_output.txt.nosanitize
		perl @PROJECT_SOURCE_DIR@/tests/fixtures/escape_sequence_sanitize.pl clean_output.txt.nosanitize > clean_output.txt
	fi
}

count_for_loops() {
	numloops=$(grep -c "FOR " $1) || true	# grep -c can return non-zero status in case of no match hence the need for "|| true"
	if [[ $numloops -ne $2 ]]; then
		echo "Expected $2 FOR loops but found $numloops loops instead in $1"
		return -1
	fi
	return
}

count_num_occurrences() {
	numoccurrences=$(grep -c "$1" $2) || true # grep -c can return non-zero status in case of no match hence the need for "|| true"
	if [[ $numoccurrences -ne $3 ]]; then
		echo "Expected $3 occurrences of [$1] but found $numoccurrences occurrences instead in $2"
		return -1
	fi
	return
}

choose_jdbc_protocol_randomly() {
	# Randomly choose simple or extended query protocol
	if [[ $(( $RANDOM % 2)) == 0 ]]; then
		echo "usesimple"
	else
		echo "useextended"
	fi
}

run_query_in_octo_and_postgres_and_crosscheck_multiple_queries() {
	# Query file has multiple queries.
	# Split it into multiple query files each containing one query and invoke "run_query_in_octo_and_postgres_and_crosscheck()".
	# Spliting queries are needed as otherwise Postgres and Octo output cannot be easily compared.

	# Init
	database="$1"
	queryfile="$2"
	psql_port="5432"
	if [[ $3 =~ ^[0-1]$ || $3 =~ ^[0]*\.?[0-9]+$ ]]; then
		# $3 is 0 or 1 (integer form) or a decimal number between 0.0 and 1.0.
		# If so, it is the "fraction" of queries. Note that down.
		# (e.g. 0.50 implies test only a random 50% of the queries)
		fraction="$3"
	else
		# $3 is not a fraction between 0.0 and 1.0. If so, include ALL queries i.e. 100% == 1.0
		fraction="1.0"
	fi

	local num_clients
	# psql errors will never have the same output as octo errors, so they cannot be used in crosscheck.
	# Therefore any error should fail the test.
	postgres_version="$(psql --command='show server_version;' --no-align --tuples-only postgres)"
	local psql_cmd
	if [ "$(echo "$postgres_version" | cut -d . -f 1)" -lt 10 ]; then
		psql_cmd=psql
		num_clients=2
	else
		psql_cmd="psql -v ON_ERROR_STOP=1"
		num_clients=3
	fi
	# Setting the following variable to 1 results in error output formatting and comparison
	enableErrorCheck=0
	# Process parameters
	for i in "$@"; do
		if [[ $i == "trim_trailing_zeroes" ]]; then
			trimzeroes="trim_trailing_zeroes"
		fi
		if [[ $i == "usejdbc" ]]; then
			usejdbc="usejdbc"
			randclientstr="JDBC"	# Since the user specified "usejdbc", fixate random client choice to JDBC
		fi
		if [[ $i == "noextended" ]]; then
			# "noextended" is specified by the caller whenever they want to disable "extended" query protocol
			# in case JDBC is chosen as the client. This is because the caller test relies on emitted M code
			# which can be different for queries executed through Extended query protocol due to the addition
			# of a LIMIT keyword (YDBOcto#1014).
			noextended=1
		fi
		if [[ $i == "usesimple" ]]; then
			jdbcprotocol="usesimple"
		fi
		if [[ $i == "useextended" ]]; then
			jdbcprotocol="useextended"
		fi
		if [[ $i == "useocto" ]]; then
			randclientstr="OCTO"
		fi
		if [[ $i == "write" ]]; then
			enableErrorCheck=1
		fi
		if [[ $i =~ ^roctoflags= ]]; then
			roctoflags=`echo $i | sed 's/^roctoflags=//g'`
		fi
	done
	if [[ ($usejdbc == "usejdbc") && ("$jdbcprotocol" == "") ]]; then
		# User specified "usejdbc" but did not explicitly specify "usesimple" or "useextended"
		# Randomly choose simple or extended query protocol
		jdbcprotocol=$(choose_jdbc_protocol_randomly)
	fi
	if [[ $randclientstr == "" ]]; then
		# If client selection was not specified in any parameters, randomly select client
		randclient=$(( $RANDOM % $num_clients))
		# Setup client parameters based on the selection
		if [[ $randclient == 2 ]]; then
			randclientstr="JDBC"
			# Randomly choose simple or extended query protocol
			jdbcprotocol=$(choose_jdbc_protocol_randomly)
			if [[ $noextended == 1 ]]; then
				# The user specified "noextended". Ensure only simple (not extended) query protocol is chosen.
				jdbcprotocol="usesimple"
			fi
		elif [[ $randclient == 1 ]]; then
			randclientstr="PSQL"
			create_default_user
		else
			randclientstr="OCTO"
		fi
	fi
	if [[ $randclientstr != "OCTO" ]]; then
		# Random client choice is PSQL or JDBC. Both these choices require the rocto server to be running.
		if [[ -z "$test_port" ]]; then
			# test_port not set in subtest, but random client choice requires starting rocto so do it
			# internally in the test framework. But also set "isnotexternal" to inform further script
			# in test framework to invoke "stop_rocto" on test completion or failure (that way the
			# caller test script (which invokes "run_query_in_octo_and_postgres_and_crosscheck_multiple_queries")
			# sees "rocto" shut down after the call just like it was before the call.
			test_port=$(start_rocto 1339 "$roctoflags")
			isnotexternal=1
		fi
	fi
	echo "Cross check parameters:" >> dbg_env.out
	echo "postgres_version:$postgres_version randclient:$randclientstr usejdbc:$usejdbc jdbcprotocol:$jdbcprotocol enableErrorCheck:$enableErrorCheck" >> dbg_env.out

	if [[ ! -f $queryfile ]]; then
		if [[ ! -f @PROJECT_SOURCE_DIR@/tests/fixtures/$queryfile ]]; then
			echo "Query file: $queryfile doesn't exist"
			exit 1
		fi
		cp @PROJECT_SOURCE_DIR@/tests/fixtures/$queryfile .
	fi
	fname=`echo $queryfile | sed 's/\..*//g'`
	# Prepare query file/files for execution
	if [[ $randclientstr == "JDBC" ]]; then
		# Remove commented instructions like -- sort-needed-check or -- rowcount-only-check
		# and commented queries as they are not handled well by JDBC client
		sed 's/\s*--.*//' $queryfile &> "$fname"_nocomment.sql
		sed 's/^\s*--.*//' $queryfile &> "$fname"_comment.sql
		# Maintaining two copies of the query file with one having comments is required to facilitate
		# result comparison in run_query_verify. The "*_nocomment.sql" files are passed to the JDBC client
		# for actual execution. But the corresponding "*_comment.sql" files are needed for later cross check
		# to process flags like "-- rowcount-only-check" etc. But if "$fraction" is specified, we cannot use
		# it in both the calls below as that would mean a different set of "*_comment.sql" and "*_nocomment.sql"
		# files could be generated in the calls due to the randomization. What we want is that for every
		# "*_nocomment.sql" file that is randomly generated, the corresponding "*_comment.sql" file exists.
		# An easy way to ensure this is to generate all "*_comment.sql" files but only a fraction of the
		# "*_nocomment.sql" files. Hence the use of "$fraction" only in the "*_nocomment.sql" call below.
		@PROJECT_SOURCE_DIR@/tests/fixtures/sqllogic/split_queries.py "$fname"_comment.sql
		@PROJECT_SOURCE_DIR@/tests/fixtures/sqllogic/split_queries.py "$fname"_nocomment.sql "" "$fraction"
		# Only $fname is passed as it acts as a search pattern for run_multiple_query_files
		# to find all query files.
		run_query_in_octo_and_postgres_and_crosscheck "$fname"
	else
		@PROJECT_SOURCE_DIR@/tests/fixtures/sqllogic/split_queries.py $queryfile "" "$fraction"
		for splitfile in $fname-*.sql
		do
			run_query_in_octo_and_postgres_and_crosscheck $splitfile
		done

	fi

	# Stop Rocto when started by the test framework in parameter processing phase
	run_query_stop_rocto
	return
}

# Note that the sed statements related to query generator update queries in this function
# are similar to sed statements in test-auto-upgrade job script in build.sh. Any changes
# here must be done there as well.
run_query_in_octo_and_postgres_and_crosscheck() {
	if [[ $usejdbc == "usejdbc" || $randclientstr == "JDBC" ]]; then
		local fname="$1"
		run_java run_multiple_query_files $psql_port "$fname"_nocomment- "$jdbcprotocol" ".psql.out" $database
		run_java run_multiple_query_files $test_port "$fname"_nocomment- "$jdbcprotocol" ".octo.out"

		local query_file
		for nocomment_query_file in "$fname"_nocomment-*.sql
		do
			# Note that "run_multiple_query_files.java" produces output even for DML/DDL commands (e.g. INSERT INTO)
			# and not just for "SELECT" so it is safe to assume that the *.psql.out and *.octo.out files exist.
			# Postgres displays boolean values as t/f whereas Octo displays them as 1/0 so fix that before the diff.
			mv $nocomment_query_file.octo.out $nocomment_query_file.octo.out.tmp
			querytype=`awk 'NR==1 {print tolower($1);}' $nocomment_query_file`
			if [[ 1 -eq $enableErrorCheck ]] && [[ "insert" == $querytype || "update" == $querytype ]]; then
				sed 's/ : \(Failing.*\)//;s/ : \(Key .*\)//;/WARN/d;s/\(Duplicate Key Value violates UNIQUE constraint\).*/\1/;s/ERROR: New row for table \(.*\) violates CHECK constraint.*/ERROR: New row for table \U\1\E violates check constraint/;' $nocomment_query_file.octo.out.tmp >& $nocomment_query_file.octo.out
				sed 's/.*new row for relation \"\(.*\)\" violates check constraint.*/ERROR: New row for table \U\1\E violates check constraint/;s/.*duplicate key value violates unique constraint.*/ERROR: Duplicate Key Value violates UNIQUE constraint/;s/\<id\([0-9]\)\>/ID\1/g;s/^DETAIL:  //;/Detail:/d;/NOTICE/d;/^Key /d;/^Failing row contains/d;s/ERROR: null value in column \"\([^"]*\)\"\( of relation \"\([^"]*\)\"\)\{0,1\} violates not-null constraint/ERROR: NULL value in column \L\1\E violates NOT NULL constraint/;s/^ERROR: column \"\(.*\)\" specified more than once/ERROR: Column '\''\U\1\E'\'' specified more than once/;/Position:/d' $nocomment_query_file.psql.out >& $nocomment_query_file.ref
			else
				cat $nocomment_query_file.octo.out.tmp | sed 's/|t$/|1/g;s/^t|/1|/g;s/|t|/|1|/g;s/^t$/1/g;s/|f$/|0/g;s/^f|/0|/g;s/|f|/|0|/g;s/^f$/0/g;' | sed 's/|t$/|1/g;s/^t|/1|/g;s/|t|/|1|/g;s/^t$/1/g;s/|f$/|0/g;s/^f|/0|/g;s/|f|/|0|/g;s/^f$/0/g;' >& $nocomment_query_file.octo.out
				cat $nocomment_query_file.psql.out | sed 's/|t$/|1/g;s/^t|/1|/g;s/|t|/|1|/g;s/^t$/1/g;s/|f$/|0/g;s/^f|/0|/g;s/|f|/|0|/g;s/^f$/0/g;' | sed 's/|t$/|1/g;s/^t|/1|/g;s/|t|/|1|/g;s/^t$/1/g;s/|f$/|0/g;s/^f|/0|/g;s/|f|/|0|/g;s/^f$/0/g;' >& $nocomment_query_file.ref
			fi
			query_num=`echo $nocomment_query_file | sed 's/.*_nocomment-//g'`
			query_file="$fname"_comment-"$query_num"
			# Distinction between commented and non-commented query file is required
			# because output processing utilizes commented instructions for Octo Postgres comparison
			run_query_verify $query_file $nocomment_query_file
		done
	else
		query_file="$1"
		local fname=`echo $query_file | sed 's/\..*//g'`
		# Additionally, DROP TABLE IF EXISTS queries issue a NOTICE that the table does not exist. We do not want this
		# as it makes comparing Postgres vs Octo output harder so disable that using the PGOPTIONS variable below.
		PGOPTIONS='--client-min-messages=warning' $psql_cmd --no-align $database -f $query_file >& $fname.psql.out || true
		# Postgres displays boolean values as t and f whereas Octo displays them as 1 and 0 so fix that before the diff.
		if [[ $randclientstr == "PSQL" ]]; then
			# Following invocation result need to be captures in a way ERROR messages are also included in the final result verification in case of INSERT/UPDATE
			# Error information is written to $fname.octo.out.err. This file will also have INFO related messages so only copy lines starting with ERROR to $fname.octo.out.tmp
			run_psql $test_port $query_file --no-align 1> $fname.octo.out.tmp 2> $fname.octo.out.err || true
			sed '/^ERROR/!d' $fname.octo.out.err >> $fname.octo.out.tmp
		else
			# OCTO
			octo -f $query_file >& $fname.octo.out.tmp || true
		fi
		# If it is a SELECT or VALUES query, then trim header information from Postgres and Octo output for effective
		# comparison. This is because the column names in the select column list might not match between Octo and Postgres
		# most of the times (sometimes they might match, but be in different case etc.). Note that SET operations might
		# start with "(SELECT ...) UNION (...)" therefore you need to also check for "(select" below.
		# For all other queries like DROP TABLE, CREATE TABLE, INSERT INTO, DELETE FROM, no need to do this trimming.

		# To get the consecutive `f`'s replacement accurate different placement variations are considered and double
		# invocation of `sed` is performed. Refer https://gitlab.com/YottaDB/DBMS/YDBOcto/-/merge_requests/977#note_732139543
		# for more details.
		querytype=`awk 'NR==1 {print tolower($1);}' $query_file`
		if [[ 1 -eq $enableErrorCheck ]] && [[ "insert" == $querytype || "update" == $querytype ]]; then
			if [[ $randclientstr == "PSQL" ]]; then
				# Two spaces after ERROR for CHECK CONSTRAINT
				sed 's/ : \(Failing.*\)//;s/ : \(Key .*\)//;/WARN/d;s/\(Duplicate Key Value violates UNIQUE constraint\).*/\1/;s/^ERROR:  New row for table \(.*\) violates CHECK constraint.*/ERROR:  New row for table \U\1\E violates check constraint/;/LINE/d;/\^/d;' $fname.octo.out.tmp &> $fname.octo.out
				# When the client is PSQL Octo would not have generated [ERROR\]: ERR_DUPLICATE_KEY_VALUE. In order to adhere to Octo's output in this case the below pattern is used.
				# This pattern is different from the pattern of sed command in else block. This difference needs to be removed once both clients have the same output.
				# two spaces after ERROR for CHECK CONSTRAINT
				sed 's/^psql.*new row for relation \"\(.*\)\" violates check constraint.*/ERROR:  New row for table \U\1\E violates check constraint/;s/^psql.*duplicate key value violates unique constraint.*/ERROR:  Duplicate Key Value violates UNIQUE constraint/;s/\<id\([0-9]\)\>/ID\1/g;s/^DETAIL:  //;/NOTICE/d;/^Key /d;/^Failing row contains/d;s/^psql.*ERROR:  null value in column \"\([^"]*\)\"\( of relation \"\([^"]*\)\"\)\{0,1\} violates not-null constraint/ERROR:  NULL value in column \L\1\E violates NOT NULL constraint/;s/^psql.*ERROR:  column \"\(.*\)\" specified more than once/ERROR:  Column '\''\U\1\E'\'' specified more than once/;/LINE/d;/\^/d' $fname.psql.out &> $fname.ref
			else
				sed 's/ : \(Failing.*\)//;s/ : \(Key .*\)//;/WARN/d;s/\(Duplicate Key Value violates UNIQUE constraint\).*/\1/;s/^\[ERROR\]: ERR_CHECK_CONSTRAINT_VIOLATION: New row for table \(.*\) violates CHECK constraint.*/\[ERROR\]: ERR_CHECK_CONSTRAINT_VIOLATION: New row for table \U\1\E violates check constraint/;/LINE/d;/\^/d;' $fname.octo.out.tmp &> $fname.octo.out
				sed 's/^psql.*new row for relation \"\(.*\)\" violates check constraint.*/\[ERROR\]: ERR_CHECK_CONSTRAINT_VIOLATION: New row for table \U\1\E violates check constraint/;s/^psql.*duplicate key value violates unique constraint.*/\[ERROR\]: ERR_DUPLICATE_KEY_VALUE: Duplicate Key Value violates UNIQUE constraint/;s/\<id\([0-9]\)\>/ID\1/g;s/^DETAIL:  //;/NOTICE/d;/^Key /d;/^Failing row contains/d;s/.*null value in column \"\([^"]*\)\"\( of relation \"\([^"]*\)\"\)\{0,1\} violates not-null constraint/\[ERROR\]: ERR_NULL_COL_VALUE: NULL value in column \L\1\E violates NOT NULL constraint/;s/^psql.*ERROR:  column \"\(.*\)\" specified more than once/\[ERROR\]: ERR_DUPLICATE_COLUMN: Column '\''\U\1\E'\'' specified more than once/;/LINE/d;/\^/d' $fname.psql.out &> $fname.ref
			fi
		elif [[ "select" == $querytype || "(select" == $querytype || "values" == $querytype ]]; then
			tail -n +2 $fname.octo.out.tmp | sed 's/|t$/|1/g;s/^t|/1|/g;s/|t|/|1|/g;s/^t$/1/g;s/|f$/|0/g;s/^f|/0|/g;s/|f|/|0|/g;s/^f$/0/g;' | sed 's/|t$/|1/g;s/^t|/1|/g;s/|t|/|1|/g;s/^t$/1/g;s/|f$/|0/g;s/^f|/0|/g;s/|f|/|0|/g;s/^f$/0/g;' &> $fname.octo.out
			tail -n +2 $fname.psql.out | sed 's/|t$/|1/g;s/^t|/1|/g;s/|t|/|1|/g;s/^t$/1/g;s/|f$/|0/g;s/^f|/0|/g;s/|f|/|0|/g;s/^f$/0/g;' | sed 's/|t$/|1/g;s/^t|/1|/g;s/|t|/|1|/g;s/^t$/1/g;s/|f$/|0/g;s/^f|/0|/g;s/|f|/|0|/g;s/^f$/0/g;' &> $fname.ref
		else
			sed 's/|t$/|1/g;s/^t|/1|/g;s/|t|/|1|/g;s/^t$/1/g;s/|f$/|0/g;s/^f|/0|/g;s/|f|/|0|/g;s/^f$/0/g;' $fname.octo.out.tmp | sed 's/|t$/|1/g;s/^t|/1|/g;s/|t|/|1|/g;s/^t$/1/g;s/|f$/|0/g;s/^f|/0|/g;s/|f|/|0|/g;s/^f$/0/g;' &> $fname.octo.out
			sed 's/|t$/|1/g;s/^t|/1|/g;s/|t|/|1|/g;s/^t$/1/g;s/|f$/|0/g;s/^f|/0|/g;s/|f|/|0|/g;s/^f$/0/g;' $fname.psql.out | sed 's/|t$/|1/g;s/^t|/1|/g;s/|t|/|1|/g;s/^t$/1/g;s/|f$/|0/g;s/^f|/0|/g;s/|f|/|0|/g;s/^f$/0/g;' >& $fname.ref
		fi
		run_query_verify $query_file $fname
	fi
	return
}

sort_LC_ALL_C() {
	# We have noticed that setting LC_ALL to en_US.UTF8 (done by the test framework at start of each subtest in "init_test()")
	# results in different sort output. This is because in UTF-8 mode, 'a' or 'A' sorts BEFORE 'b' and 'B' whereas in
	# C/ASCII mode, 'a' and 'b' sorts AFTER 'A' and 'B'. Therefore, we work around this issue by setting the locale to C just
	# for the duration of the sort. This lets us have deterministic reference files.
	export LC_ALL=C
	sort
	export LC_ALL=en_US.UTF8
}

run_query_verify() {
	# Ensure no errors are seen in stderr.txt
	verify_no_octo_or_ydb_error
	local query_file="$1"
	local fname="$2"
	sed -i '/^.*\[ INFO\].* \/.*\/octo.conf/d' $fname.octo.out
	cat $fname.octo.out >& $fname.log
	if [[ $trimzeroes == "trim_trailing_zeroes" ]]; then
		# Remove floating point numbers with extraneous 0s at the end
		# Postgres creates these floating point numbers if an AVG aggregate function is invoked whereas Octo does not.
		mv $fname.ref $fname.ref1
		mv $fname.log $fname.log1
		# The below step converts for example : 2.5000000000000000 -> 2.5
		pat1='s/\(\.[0-9]*[1-9]\)0\{1,\}$/\1/;s/\(\.[0-9]*[1-9]\)0\{1,\}|/\1|/g;'
		sed $pat1 $fname.ref1 > $fname.ref2
		sed $pat1 $fname.log1 > $fname.log2
		# The below step converts for example : 1.0000000000000000 -> 1
		pat2='s/\.0\{1,\}$//;s/\.0\{1,\}|/|/g;'
		sed $pat2 $fname.ref2 > $fname.ref3
		sed $pat2 $fname.log2 > $fname.log3
	else
		# If $trimzeroes != "trim_trailing_zeroes", still move the files into their "ref3" and "log3"
		# versions for use later on in the process.
		mv $fname.ref $fname.ref3
		mv $fname.log $fname.log3
	fi
	# Below sed commands add leading zeros into the Octo output in order to match PSQL's output
	#	Octo Output: .333   PSQL Output: 0.333
	#	Octo Output: -.333   PSQL Output: -0.333
	sed 's/|\.\([0-9]*\)/|0.\1/;s/|-\.\([0-9]*\)/|-0.\1/;' $fname.log3 > $fname.log4
	sed 's/^\.\([0-9]*\)/0.\1/;s/^-\.\([0-9]*\)/-0.\1/;' $fname.log4 > $fname.log5
	# Check if query has ORDER BY that is not inside a subquery OR if "--sort-needed-check" is present. If so, do not sort.
	# Otherwise sort as order is not guaranteed to match between Octo and Postgres.
	# grep -c can return non-zero status in case of no match hence the need for "|| true"
	orderbyexists=$(yottadb -r removeInnerQueries $query_file | grep -ci "ORDER BY") || true
	sortneededcheck=$(grep -c "sort-needed-check" $query_file) || true
	if [[ $orderbyexists -eq 0 ]] || [[ $sortneededcheck -ne 0 ]]; then
		# ORDER BY does not exist in the query. So sort the outputs.
		mv $fname.ref3 $fname.unsorted.ref3
		mv $fname.log5 $fname.unsorted.log5
		cat $fname.unsorted.ref3 | sort_LC_ALL_C > $fname.ref3
		cat $fname.unsorted.log5 | sort_LC_ALL_C > $fname.log5
	fi
	# Below two seds set numerical values to only have 10 digits after the decimal point as both Octo
	# and PSQL can return varying levels of precision (12 is the least amount of decimals seen so far)
	sed -re 's/([0-9]+\.[0-9]{10})[0-9]+/\1/g' $fname.ref3 >& $fname.ref
	sed -re 's/([0-9]+\.[0-9]{10})[0-9]+/\1/g' $fname.log5 >& $fname.log
	# grep -c can return non-zero status in case of no match hence the need for "|| true"
	rowcountonlycheck=$(grep -c "rowcount-only-check" $query_file) || true
	if [[ $rowcountonlycheck -eq 0 ]]; then
		# diff result is captured to stop_rocto before issuing a test fail
		diff $fname.ref $fname.log >& $fname.diff || true
		if [[ -s $fname.diff ]]; then
			run_query_stop_rocto
			exit 1
		fi
	else
		# .sql file requests only rowcount check. No exact content check.
		# Check only # of lines in output match. Not contents of lines.
		psqllines=$(wc -l $fname.ref | awk '{print $1}')
		octolines=$(wc -l $fname.log | awk '{print $1}')
		if [[ $psqllines -ne $octolines ]]; then
			echo "Expected $psqllines lines but found $octolines lines in $fname.log"
			run_query_stop_rocto
			exit 1
		fi
	fi
}

run_query_stop_rocto() {
	# This function is only used inside the test framework
	if [[ $randclientstr != "OCTO" ]]; then
		if [[ ! -z $isnotexternal ]];then
			stop_rocto
			# Reset rocto port related variables so the next "if [[ -z "$test_port" ]];"
			# check in "run_query_in_octo_and_postgres_and_crosscheck_multiple_queries"
			# function will correctly invoke "start_rocto" like it should.
			isnotexternal=
			test_port=
		fi
	fi
	randclientstr=
}

run_query_generator() {
  databaseName="$1"
  prefix="$2"
  usejdbcoption="$3"
  useextended="$4"
  numQueries=100	# For now generate 100 queries by default (good balance of test run time vs test coverage)

  if [[ "" != $usejdbcoption ]]; then
	numQueries=$(( $numQueries / 4 ))	# Generate 25% of queries to save time in the pipeline for JDBC tests
  fi

  cp @PROJECT_BINARY_DIR@/configured_fixtures/QueryGenerator.m .
  if [[ "nameslastname" = $databaseName ]]; then
	load_fixture "postgres-$databaseName.sql"
  	databaseSqlFileName="postgres-$databaseName.sql"
  else
	load_fixture "$databaseName.sql"
	load_fixture "$databaseName.zwr"
  	databaseSqlFileName="$databaseName.sql"
  fi
  load_fixture "QueryGenerator.sql"
  # 80% of the time select read-only mode and 20% of the time select read-write mode
  isReadWriteMode=$(($RANDOM % 100))
  if [[ 80 -gt $isReadWriteMode ]]; then
	isReadWriteMode=""
  else
	isReadWriteMode="write"
  fi
  $ydb_dist/yottadb -run -noignore QueryGenerator $databaseSqlFileName "$databaseName.zwr" "$numQueries" "$prefix" $isReadWriteMode 2> "QG_stderr.txt" 1> "QG_stdout.txt"

  queryfile="$prefix.sql"
  if [[ -f "tabledefinition-$queryfile" ]]; then
	# QueryGenerator.m has generated table definition files that has to be run in Octo and Postgres.
	# Run them here such that the cross check call below can work on the new table definition.
	@PROJECT_SOURCE_DIR@/tests/fixtures/sqllogic/split_queries.py  "tabledefinition-$queryfile"
	for splitfile in  tabledefinition-$prefix-*.sql
	do
		octo -f "$splitfile" >> "octo-tabledefinition-$prefix.out" 2>>"octo-tabledefinition-$prefix-err.out" || true
	done
	postgres_version="$(psql --command='show server_version;' --no-align --tuples-only postgres)"
	local psql_cmd
	if [ "$(echo "$postgres_version" | cut -d . -f 1)" -lt 10 ]; then
		psql_cmd=psql
	else
		psql_cmd="psql -v ON_ERROR_STOP=1"
	fi
	for splitfile in tabledefinition-$prefix-*.sql
	do
		PGOPTIONS='--client-min-messages=warning' $psql_cmd --no-align $databaseName -f "$splitfile" >> "postgres-tabledefinition-$prefix.out" 2>>"postgres-tabledefinition-$prefix-err.out" || true
	done
  fi
  run_query_in_octo_and_postgres_and_crosscheck_multiple_queries $databaseName $queryfile "trim_trailing_zeroes" $usejdbcoption $useextended $isReadWriteMode
  if [[ -f "tabledefinition-$queryfile" ]]; then
	octo -f "drop-tabledefinition-$queryfile" >& "octo-drop-tabledefinition-$prefix.out"
	PGOPTIONS='--client-min-messages=warning' $psql_cmd --no-align $databaseName -f "drop-tabledefinition-$queryfile" >& "postgres-drop-tabledefinition-$prefix.out"
  fi
}

run_query_in_octo_and_mysql_and_crosscheck_multiple_queries() {
	# Query file has multiple queries.
	# Split it into multiple query files each containing one query and invoke "run_query_in_octo_and_postgres_and_crosscheck()".
	# Spliting queries are needed as otherwise Postgres and Octo output cannot be easily compared.

	# Init
	database="$1"
	queryfile="$2"
	checktime="$3"

	if [[ ! -f $queryfile ]]; then
		if [[ ! -f @PROJECT_SOURCE_DIR@/tests/fixtures/$queryfile ]]; then
			echo "Query file: $queryfile doesn't exist"
			exit 1
		fi
		cp @PROJECT_SOURCE_DIR@/tests/fixtures/$queryfile .
	fi
	fname=`echo $queryfile | sed 's/\..*//g'`
	echo "Cross check parameters:" >> env.out
	echo "mysql_version:$(mysql --version)" >> env.out

	@PROJECT_SOURCE_DIR@/tests/fixtures/sqllogic/split_queries.py $queryfile
	for splitfile in $fname-*.sql
	do
		run_query_in_octo_and_mysql_and_crosscheck $splitfile $checktime
	done

	# Stop Rocto when started by the test framework in parameter processing phase
	run_query_stop_rocto
	return
}

run_query_in_octo_and_mysql_and_crosscheck() {
	query_file="$1"
	checktime="$2"
	local fname=`echo $query_file | sed 's/\..*//g'`

	# Export the MySQL user password to suppress a security warning from MySQL
	# that muddles output. While this is insecure and shouldn't be done on production
	# systems, it is acceptable in a test environment where programmatic execution
	# is desired and no sensitive data is stored in the database.
	export MYSQL_PWD="ydbrocks"
	starttime=$(date +%s) # Mark start time in case time function output is to be validated
	run_mysql $database $query_file >& $fname.mysql.out
	# MySQL displays boolean values as t and f whereas Octo displays them as 1 and 0 so fix that before the diff.
	tail -n +2 $fname.mysql.out | sed 's/\<t\>/1/g;s/\<f\>/0/g;' >& $fname.ref
	# Replace NULL with empty string to match Octo behavior
	sed -i 's/NULL//g;' $fname.ref

	# Run query in octo
	octo -e MYSQL -f $query_file >& $fname.octo.out.tmp
	endtime=$(date +%s) # Mark end time in case time function output is to be validated

	# trim header and footer information from octo output for effective comparison
	tail -n +2 $fname.octo.out.tmp | sed 's/\<t\>/1/g;s/\<f\>/0/g;' &> $fname.octo.out.tmp2
	head -n -1 $fname.octo.out.tmp2 &> $fname.octo.out

	export MYSQL_PWD=""
	# If the queries contain time-related functions, as indicated by the $checktime, check whether the queries executed within
	# the same second. If so, run the cross check. If not, skip the cross-check by returning from this function, as the
	# crosscheck is likely to fail due to a timing discrepancy, even if Octo executed the query correctly.
	if [[ "checktime" == $checktime ]]; then
		# Record all time output in case needed for debugging
		echo "Start time: $starttime" >> time.out
		echo "End time: $endtime" >> time.out
		if [[ $starttime -ne $endtime ]]; then
			echo "--> Query verification skipped for $query_file." >> time.out
			return
		fi
	fi
	run_query_verify $query_file $fname

	return
}

run_query_in_two_emulations_and_crosscheck_multiple_queries() {
	# Query file has multiple queries.
	# Split it into multiple query files each containing one query and invoke "run_query_in_octo_and_postgres_and_crosscheck()".
	# Spliting queries are needed as otherwise Postgres and Octo output cannot be easily compared.

	# Init
	database="$1"
	queryfile="$2"
	first_emulation=$3
	second_emulation=$4
	fname=`echo $queryfile | sed 's/\..*//g'`

	if [[ ! -f $queryfile ]]; then
		if [[ ! -f @PROJECT_SOURCE_DIR@/tests/fixtures/$queryfile ]]; then
			echo "Query file: $queryfile doesn't exist"
			exit 1
		fi
		cp @PROJECT_SOURCE_DIR@/tests/fixtures/$queryfile .
	fi
	echo "Cross check parameters:" >> env.out
	echo -e "emulation #1:$first_emulation\temulation #2:$second_emulation" >> env.out

	@PROJECT_SOURCE_DIR@/tests/fixtures/sqllogic/split_queries.py $queryfile
	for splitfile in $fname-*.sql
	do
		run_query_in_two_emulations_and_crosscheck $splitfile $first_emulation $second_emulation
	done

	# Stop Rocto when started by the test framework in parameter processing phase
	run_query_stop_rocto
	return
}

run_query_in_two_emulations_and_crosscheck() {
	query_file="$1"
	first_emulation=$2
	second_emulation=$3
	local fname=`echo $query_file | sed 's/\..*//g'`

	# Run query in octo, first emulation
	octo -e $first_emulation -f $query_file >& $fname.emu1.out
	# Run query in octo, second emulation
	octo -e $second_emulation -f $query_file >& $fname.emu2.out

	diff $fname.emu1.out $fname.emu2.out

	return
}

corecheck() {
	# Check if any core files exist.
	corefilelist=$(find . -name "core*")
	[[ -z $corefilelist ]]
}
